{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RatGPS_fpp_location.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim==4.1.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19fj3hdwQlSP",
        "outputId": "8ea4f4b0-bf00-4259-9fce-f791b8753ad1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.1.2 in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (5.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ij-Adq37MjjE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
        "import numpy as np\n",
        "import math\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from random import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the unprocessed raw data. both spikes and locations measured at 20ms. \n",
        "# One should be able to replicate all rat2192 feat and pos files from these sources\n",
        "raw_spikes = pd.read_csv(\"https://drive.google.com/uc?id=1-IuXanc2RZICrzqEG7jTg7f_AN9K9Ufl&export=download\", header=None)\n",
        "raw_loc = pd.read_csv(\"https://drive.google.com/uc?id=1-XHfaaYkfNeQHYefY4uGX4r1q3AKGbic&export=download\", header=None)\n",
        "\n",
        "raw_loc.columns = [\"x\", \"y\"]\n",
        "\n",
        "raw_spikes_revers = raw_spikes[::-1].reset_index(drop = True)\n",
        "raw_loc_revers = raw_loc[::-1].reset_index(drop = True)"
      ],
      "metadata": {
        "id": "buOGWN2wMmiI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_windows(df_len, window_len=10, window_hop=10, skip=0, midpos=-1, drop_incomplete=False):\n",
        "  num_windows = math.ceil((df_len - skip) / window_hop)\n",
        "  answer = []\n",
        "  for i in range(num_windows):\n",
        "    start = skip + i * window_hop\n",
        "    end = min(start + window_len, df_len)\n",
        "\n",
        "    if drop_incomplete and end-start < window_len:\n",
        "      print(f\"Dropping range {start}-{end}\")\n",
        "      break\n",
        "\n",
        "    if midpos == -1:\n",
        "      mid = math.floor((start + end) / 2)\n",
        "    else:\n",
        "      mid = start + midpos\n",
        "\n",
        "    answer.append((start, end, mid))\n",
        "\n",
        "  return answer\n",
        "\n",
        "\n",
        "\n",
        "def build_sentences(df, window_len = 70, window_hop = 10, skip = 0, max_empty_words = -1, word_ordering = \"shuffle\"):\n",
        "  \"\"\"\n",
        "  builds sentence vectors from spiking data\n",
        "  window_len: how many spiking timesteps are used for one sentence\n",
        "  window_hop: how much to slide the window. if window_len==window_hop there is no overlap. if hop<len, there is overlap.\n",
        "  max_empty_words: how many consecutive empty words (\"_\") to keep. if 0, they are removed alltogether. if -1, keep all of them\n",
        "  word_ordering: \"shuffle\" or \"sort\". If several neurons spike in one timestep, how to order them. If the same set of neurons spiked, maybe we should have consistent ordering?\n",
        "  skip: how many timesteps to skip from the beginning. maybe the rat is training at first and we should discard that data?\n",
        "  \"\"\"\n",
        "\n",
        "  sents = []\n",
        "  for w in get_windows(len(df), window_len, window_hop, skip):\n",
        "    start, end, mid = w\n",
        "\n",
        "    # process one sentence\n",
        "    sent_words = []\n",
        "    empty_seq_len = 0\n",
        "    for j in range(start, end):\n",
        "      row = df.iloc[j]\n",
        "\n",
        "      if np.sum(row) == 0:\n",
        "        empty_seq_len += 1\n",
        "\n",
        "        if max_empty_words == -1 or max_empty_words >= empty_seq_len:\n",
        "          sent_words.append(64)\n",
        "\n",
        "      else:\n",
        "        empty_seq_len = 0\n",
        "\n",
        "        one_spike = row[row == 1].index.tolist()\n",
        "        two_spikes = row[row == 2].index.tolist()\n",
        "        three_spikes = row[row == 3].index.tolist()\n",
        "        four_spikes = row[row == 4].index.tolist()\n",
        "        word = 4 * four_spikes + 3 * three_spikes + 2 * two_spikes + one_spike\n",
        "\n",
        "        if word_ordering == \"shuffle\":\n",
        "          shuffle(word)\n",
        "        else:\n",
        "          word.sort()\n",
        "\n",
        "        sent_words += word\n",
        "\n",
        "    if len(sent_words) == 0:\n",
        "      sent_words = [64]\n",
        "\n",
        "    sents.append(sent_words)\n",
        "\n",
        "  return sents\n",
        "\n",
        "\n",
        "\n",
        "def get_locations(df, window_len=70, window_hop=10, skip=0):\n",
        "    idx = [False] * len(df)\n",
        "    for w in get_windows(len(df), window_len, window_hop, skip):\n",
        "        end = w[1]\n",
        "        idx[end - 1] = True\n",
        "\n",
        "    return df[idx]"
      ],
      "metadata": {
        "id": "lT4Fqb_KMwKE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_word2vec(sents, vector_len=250, train_size=0.8, window_size=5, sg=0, shrink_windows=True, alpha=0.025,\n",
        "                   seed=42):\n",
        "  model = Word2Vec(min_count=1, vector_size=vector_len, window=window_size, max_vocab_size=None, max_final_vocab=None,\n",
        "                   sg=sg, compute_loss=True, workers=6, shrink_windows=shrink_windows, alpha=alpha, seed=seed)\n",
        "  # Build vocab from full dataset.\n",
        "  # In some cases, the word might not have ended up in train set. Then the model would throw an error.\n",
        "  model.build_vocab(sents)\n",
        "\n",
        "  train_test_boundary = math.floor(len(sents) * train_size)\n",
        "  train_sents = sents[:train_test_boundary]\n",
        "  model.train(corpus_iterable=train_sents, total_examples=len(train_sents), epochs=20, compute_loss=True)\n",
        "  return model"
      ],
      "metadata": {
        "id": "nwEe7hFfM3l7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_features(word2vec_model, sents, method=\"mean\"):\n",
        "  features = []\n",
        "  for sent in sents:\n",
        "    vecs = np.array([word2vec_model.wv[word] for word in sent])\n",
        "    if method == \"mean\":\n",
        "      features.append(np.mean(vecs, axis=0))\n",
        "    else:\n",
        "      # concatenate instead\n",
        "      features.append(vecs.flatten())\n",
        "\n",
        "  if method != \"mean\":\n",
        "    # right-pad every row with zeros so that every vector has the same length\n",
        "    maxlen = max([len(row) for row in features])\n",
        "    features = [np.append(row, [0] * (maxlen - len(row))) for row in features]\n",
        "    features = np.array(features)\n",
        "  return features"
      ],
      "metadata": {
        "id": "pKKRL7lzRxoM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_df(df, train_ratio):\n",
        "  idx = math.floor(len(df) * train_ratio)\n",
        "  return df[:idx], df[idx:]\n",
        "\n",
        "\n",
        "#wordVecs, sents, locs,\n",
        "#sent_vecs = build_features(wordVecs, sents, method = \"mean\")\n",
        "def benchmark(model: MultiOutputRegressor, X, y, train_ratio=0.8):\n",
        "  if len(X) != len(y):\n",
        "    raise Exception(f\"different number of sentences and locations: {len(X)} != {len(y)}\")\n",
        "\n",
        "  train_x, test_x = split_df(X, train_ratio)\n",
        "  train_x = pd.DataFrame(data=train_x)\n",
        "  test_x = pd.DataFrame(data=test_x)\n",
        "\n",
        "  train_y, test_y = split_df(locations, train_ratio)\n",
        "\n",
        "  model = model.fit(train_x, train_y)\n",
        "  preds = model.predict(test_x)\n",
        "  preds = preds.T\n",
        "\n",
        "  dists = np.sqrt((test_y['x'] - preds[0]) ** 2 + (test_y['y'] - preds[1]) ** 2)\n",
        "  avg_dist = np.mean(dists)\n",
        "\n",
        "  return avg_dist\n"
      ],
      "metadata": {
        "id": "5jATZovtM7me"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE_LR = []\n",
        "MSE_RFR = []"
      ],
      "metadata": {
        "id": "b-kEUKTrWVWp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3000, 10, -1):\n",
        "\n",
        "    sents = build_sentences(raw_spikes_revers, window_len=i, window_hop=i, skip=0)\n",
        "    locations = get_locations(raw_loc_revers, window_len=i, window_hop=i, skip=0)\n",
        "\n",
        "    word2vec_model = train_word2vec(sents)\n",
        "    input_features = build_features(word2vec_model, sents, method=\"mean\")\n",
        "\n",
        "    avg_dist_lr = benchmark(\n",
        "    MultiOutputRegressor(LinearRegression()),\n",
        "    X=input_features,\n",
        "    y=locations\n",
        "    )\n",
        "    MSE_LR.append(avg_dist_lr)\n",
        "\n",
        "\n",
        "    avg_dist_rfr = benchmark(\n",
        "    MultiOutputRegressor(RandomForestRegressor(min_samples_leaf=3)),\n",
        "    X=input_features,\n",
        "    y=locations\n",
        "    )\n",
        "    MSE_RFR.append(avg_dist_rfr)"
      ],
      "metadata": {
        "id": "bQxi8WwlM-zE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10, 3000):\n",
        "\n",
        "    sents = build_sentences(raw_spikes, window_len=i, window_hop=i, skip=0)\n",
        "    locations = get_locations(raw_loc, window_len=i, window_hop=i, skip=0)\n",
        "\n",
        "    word2vec_model = train_word2vec(sents)\n",
        "    input_features = build_features(word2vec_model, sents, method=\"mean\")\n",
        "\n",
        "    avg_dist_lr = benchmark(\n",
        "    MultiOutputRegressor(LinearRegression()),\n",
        "    X=input_features,\n",
        "    y=locations\n",
        "    )\n",
        "    MSE_LR.append(avg_dist_lr)\n",
        "    \n",
        "\n",
        "    avg_dist_rfr = benchmark(\n",
        "    MultiOutputRegressor(RandomForestRegressor(min_samples_leaf=3)),\n",
        "    X=input_features,\n",
        "    y=locations\n",
        "    )\n",
        "    MSE_RFR.append(avg_dist_rfr)\n"
      ],
      "metadata": {
        "id": "VEOo6omVWKIQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE_LR, MSE_RFR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGxQemgSMsLu",
        "outputId": "4f90ef70-1db0-4bd5-e3b2-6bc91f0a5b41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([], [])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}