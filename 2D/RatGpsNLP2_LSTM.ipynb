{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c290b697",
   "metadata": {},
   "source": [
    "# NeuroNLP - new experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "009aa050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/space/home/renata24\n"
     ]
    }
   ],
   "source": [
    "#!pip install gensim==4.1.2\n",
    "#%cd ..\n",
    "#%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053f63c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPUs: 64\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "pool = multiprocessing.Pool()\n",
    "print('CPUs:', pool._processes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf9990f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs:  1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    workers = torch.cuda.device_count() - 1  #???\n",
    "    print('GPUs: ', torch.cuda.device_count())    \n",
    "else:\n",
    "  print('No GPUs...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6346b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec  \n",
    "from gensim.models import KeyedVectors \n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import pickle as plk\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from wandb.keras import WandbCallback\n",
    "#import wandb\n",
    "from tensorflow.keras.losses import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d752c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_linux = True\n",
    "\n",
    "# File separator:\n",
    "if is_linux:\n",
    "    s = \"/\"  # linux\n",
    "else:\n",
    "    s = \"\\\\\" # windows\n",
    "\n",
    "path_to_data = \"data\" + s\n",
    "#path_to_NLP = \"NLP\" + s\n",
    "path_to_w2v = \"word2vec\" + s\n",
    "path_to_results = \"NLP_results\" +s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549844f2",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e0f624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fname):\n",
    "    with open(fname, \"r\", encoding=\"UTF-8\") as f:\n",
    "        data = [line.rstrip().split(' ') for line in f.readlines()]\n",
    "    df = pd.DataFrame(data = data)\n",
    "    if len(df.columns) == 2:\n",
    "        df.columns = ['x', 'y']\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].astype('float32')\n",
    "    else:\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].astype('int64')        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcea3fb",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab76b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sents(dfx, locs, window_size, step, repetitions, do_shuffle): # window size - No of 20 ms intervals \n",
    "    sents = []\n",
    "    spikes = [] # for comparison\n",
    "    start, end = 0, window_size\n",
    "    empty_windows = 0\n",
    "    x_new, y_new = [], []\n",
    "    x, y = locs['x'], locs['y']\n",
    "    \n",
    "    while end < len(dfx):\n",
    "        \n",
    "        # Sentences: \n",
    "        rows = dfx.iloc[start:end] # rows in window\n",
    "        row = np.sum(rows) # all spikes of each neuron in window\n",
    "        sent_words = []\n",
    "        \n",
    "        if np.sum(row)==0:\n",
    "            empty_windows+=1\n",
    "            sents.append(sent_words)\n",
    "        else: # if there were any spikes at all\n",
    "            for j, spike_count in enumerate(row):\n",
    "                if spike_count!=0:\n",
    "                    if repetitions==True:                    \n",
    "                        sent_words+=[row.index[j] for x in range(spike_count)]                             \n",
    "                    else:\n",
    "                        sent_words.append(row.index[j])\n",
    "            if do_shuffle==True:\n",
    "                shuffle(sent_words)\n",
    "            sents.append(sent_words)         \n",
    "        \n",
    "        # Spikes:\n",
    "        spikes.append(row.tolist())    \n",
    "            \n",
    "        # Locations:\n",
    "        if window_size==1:\n",
    "            loc_x, loc_y = x[start], y[start]\n",
    "        elif window_size%2==0: # even number\n",
    "            loc_ind = int(start+(window_size)/2)\n",
    "            loc_ind2 =  loc_ind-1\n",
    "            loc_x = (x[loc_ind] + x[loc_ind2])/2\n",
    "            loc_y = (y[loc_ind] + y[loc_ind2])/2\n",
    "        else: # odd number\n",
    "            loc_ind = int(start+(window_size-1)/2)\n",
    "            loc_x, loc_y = x[loc_ind], y[loc_ind]\n",
    "        x_new.append(loc_x)\n",
    "        y_new.append(loc_y)\n",
    "        start+=step\n",
    "        end+=step\n",
    "     \n",
    "    locs2 = pd.DataFrame(data=[x_new, y_new]).T\n",
    "    locs2.columns = ['x', 'y']   \n",
    "    \n",
    "    return [sents, locs2, empty_windows, spikes] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b5545d",
   "metadata": {},
   "source": [
    "## Analysis of goodness of windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5100b",
   "metadata": {},
   "source": [
    "How many empty and total sentences with different window size and step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8412aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_windows(dfx, locations2, window_sizes, steps, repetitions):\n",
    "    for i in range(len(steps)):\n",
    "        window_size = window_sizes[i]\n",
    "        step = steps[i]\n",
    "        \n",
    "        sents, locations3, empty_windows, spikes = make_sents(dfx, locations2, window_size=window_size, step=step, repetitions=repetitions, do_shuffle=False)\n",
    "        sent_lens = pd.Series([len(s) for s in sents]).value_counts()\n",
    "        print('\\nwindow_size=', window_size, '(', window_size*20,'ms)', 'step=', step, '(', step*20,'ms)')\n",
    "        print('empty windows: ', empty_windows, 'out of', len(sents), '(', round(100*empty_windows/(len(sents)), 2),'%), non-empty:', len(sents)-empty_windows)\n",
    "        print(sent_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8ba64",
   "metadata": {},
   "source": [
    "## Calculate weights for neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf443891",
   "metadata": {},
   "source": [
    "Weight depends on how disperse or compact is the neuron's receptive field. Neurons with disperse fields receive lower weight in sentence, and those with more compact receptive field receive larger weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b471fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weights(dfx, locations2x, avg_loc):\n",
    "    \n",
    "    # Calculate centroid of each receptive field:\n",
    "    neurons = dfx.columns.tolist()\n",
    "    df_all = pd.concat([dfx, locations2x], axis=1)\n",
    "    centroids_x, centroids_y = [], []\n",
    "    centroids_y= []\n",
    "    spike_times = []\n",
    "    for neuron in neurons:\n",
    "        d = df_all[df_all[neuron]>0]\n",
    "        # d = df_all[df_all['42']>0]\n",
    "        spike_times.append(len(d))\n",
    "        if len(d)==0:  # if neuron didn't spike, assume it's centroid to be the default average location\n",
    "            centroids_x.append(avg_loc[0])\n",
    "            centroids_y.append(avg_loc[1])\n",
    "        else:\n",
    "            d = d[[neuron, 'x', 'y']]\n",
    "            centroids_x.append(np.sum(d[neuron]*d['x'])/np.sum(d[neuron]))\n",
    "            centroids_y.append(np.sum(d[neuron]*d['y'])/np.sum(d[neuron]))\n",
    "    df_centroids = pd.DataFrame(data = [neurons, centroids_x, centroids_y, spike_times]).T\n",
    "    df_centroids.columns = ['neuron', 'x', 'y', 'spike_times']\n",
    "    df_centroids.index = neurons\n",
    "    \n",
    "    # Average distance of spike locations to centroid:\n",
    "    means = []\n",
    "    st_devs = []\n",
    "    for neuron in neurons:\n",
    "        d = df_all[df_all[neuron]>0]\n",
    "        if len(d)==0: \n",
    "            means.append(45.0) # to have the missing neuron have very low weight\n",
    "            st_devs.append(30.0)   # stdev is now not actually used...    \n",
    "        else:\n",
    "            d = d[[neuron, 'x', 'y']]\n",
    "            # centroid = df_centroids.iloc[int(neuron)]\n",
    "            centroid = df_centroids.loc[neuron]\n",
    "            dists = np.sqrt((d['x']-centroid['x'])**2+(d['y']-centroid['y'])**2)\n",
    "            avg_dist = np.mean(dists)  \n",
    "            stdev_dists = np.std(dists)\n",
    "            # plt.hist(dists, bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110])\n",
    "            # plt.xlim(0,110)\n",
    "            means.append(avg_dist)\n",
    "            st_devs.append(stdev_dists) \n",
    "    dists_df = pd.DataFrame(data = [means, st_devs, spike_times]).T\n",
    "    dists_df.columns = ['avg_dist', 'std_dist', 'spike_times']\n",
    "    # plt.plot(sorted(dists_df['avg_dist']))\n",
    "    \n",
    "    dists_df.index = pd.Series(neurons).astype('int64')\n",
    "    \n",
    "    # Scale the distances:\n",
    "    a = np.max(means)-means # inverse # 0- worst, 33.3- best\n",
    "    # plt.plot(sorted(a)) # \n",
    "    a2 = (a/np.max(a))+0.1  # to avoid zero \n",
    "    \n",
    "    # TODO: When assigning weights, take better into account how many times the neuron \n",
    "    # spiked - if it spiked only a few times, its weight should be lower (because then \n",
    "    # we have less confidence that in future it will spike in similar locations). \n",
    "    # - For now, just multiply weight by 0.9 if there were less than 10 spikes:\n",
    "    b = pd.Series(spike_times)<10\n",
    "    a2 = pd.Series(a2).where(-b, a2*0.9) \n",
    "\n",
    "    dists_df['weight'] = a2    \n",
    "    dists_df['weight^3'] = a2**3 # to give more weight to more compact receptive fields\n",
    "    dists_df['w+2*w^3']=dists_df['weight'] + dists_df['weight^3']*2 \n",
    "    \n",
    "    # plt.plot(sorted(dists_df['weight'] ))\n",
    "    # plt.plot(sorted(dists_df['weight^3'] ))    \n",
    "    # plt.plot(sorted(dists_df['w+2*w^3']))\n",
    "\n",
    "    return dists_df\n",
    "\n",
    "# dists_df = calc_weights(df, locations2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0262074c",
   "metadata": {},
   "source": [
    "## Train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed4d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test(sents, locs2, spikes, train_index, test_index, dfx, locations2, \n",
    "                    win_size, step, rep, do_shuffle, remove_duplicates, fold):\n",
    "        \n",
    "    test_locs_tmp = locs2.iloc[test_index]\n",
    "    train_locs_tmp = locs2.iloc[train_index]\n",
    "    test_sents_tmp = pd.Series(sents).iloc[test_index].tolist()\n",
    "    train_sents_tmp = pd.Series(sents).iloc[train_index].tolist()\n",
    "    test_spikes_tmp = pd.Series(spikes).iloc[test_index].tolist()\n",
    "    train_spikes_tmp =pd.Series(spikes).iloc[train_index].tolist()\n",
    "\n",
    "    # Average location in train set (for predicting empty rows in test set):\n",
    "    avg_loc = [np.mean(train_locs_tmp['x']), np.mean(train_locs_tmp['y'])]\n",
    "    \n",
    "    # Calculate weights for neurons, based on train set:\n",
    "    # -Those indexes are based on 1/10 parts of the original dataframe (the one \n",
    "    #containing the sentences is already shorter, because we aggregated data in each window.\n",
    "    split_size = int(len(dfx)/10)  # 5410\n",
    "    if fold==0:\n",
    "        start = split_size\n",
    "        end =  len(dfx)\n",
    "        dists_df = calc_weights(dfx.iloc[start:end], locations2.iloc[start:end], avg_loc)\n",
    "    elif fold==9:\n",
    "        start = 0\n",
    "        end = len(dfx) - split_size\n",
    "        dists_df = calc_weights(dfx.iloc[start:end], locations2.iloc[start:end], avg_loc)\n",
    "    else: \n",
    "        start = 0\n",
    "        end = fold*split_size\n",
    "        start2 = fold*split_size + split_size\n",
    "        end2 = len(dfx)\n",
    "        dfx_part1 = dfx.iloc[start:end]\n",
    "        dfx_part2 = dfx.iloc[start2:end2]\n",
    "        locs_part1 = locations2.iloc[start:end]\n",
    "        locs_part2 = locations2.iloc[start2:end2]    \n",
    "        dfx_parts_all = dfx_part1.append(dfx_part2, sort = False) \n",
    "        locs_parts_all = locs_part1.append(locs_part2, sort = False) \n",
    "        dists_df = calc_weights(dfx_parts_all, locs_parts_all, avg_loc)\n",
    "    \n",
    "    # Separate empty rows and their locations from test set:\n",
    "    # - (they will be put back before calculating RMSE)\n",
    "    x_new, y_new, test_sents, test_spikes = [], [], [], []\n",
    "    x_new_empty, y_new_empty = [], []\n",
    "    x, y = test_locs_tmp['x'], test_locs_tmp['y']\n",
    "    loc_ids = x.index.tolist()\n",
    "    for i, sent in enumerate(test_sents_tmp):\n",
    "        if len(sent)!=0:\n",
    "            test_sents.append(sent)\n",
    "            test_spikes.append(test_spikes_tmp[i])\n",
    "            x_new.append(x[loc_ids[i]])\n",
    "            y_new.append(y[loc_ids[i]])\n",
    "        else:\n",
    "            x_new_empty.append(x[loc_ids[i]])\n",
    "            y_new_empty.append(y[loc_ids[i]])\n",
    "    \n",
    "    test_locs_empty = pd.DataFrame(data=[x_new_empty, y_new_empty]).T\n",
    "    test_locs_empty.columns = ['x', 'y']\n",
    "    test_locs = pd.DataFrame(data=[x_new, y_new]).T\n",
    "    test_locs.columns = ['x', 'y']\n",
    "        \n",
    "\n",
    "    # If there is same sentence in consequtive windows, leave only one such sentence, \n",
    "    #and average the location (this occurs because of moving window: if 20ms intervals \n",
    "    #at the edges of the window were empty, same sentence was included several times)\n",
    "    #- ONLY MAKES SENCE IF \"DO_SHUFFLE\" = False. BUT ACTUALLY, SINCE WE SHUFFLE, THIS \n",
    "    #HAS BECOME USELESS...\n",
    "    if remove_duplicates==True:\n",
    "        x_new, y_new, train_sents2, train_spikes2 = [], [], [], []\n",
    "        x, y = train_locs_tmp['x'], train_locs_tmp['y']\n",
    "        prev_x, prev_y = [], []\n",
    "        prev_sent = ['']\n",
    "        for i, sent in enumerate(train_sents_tmp):\n",
    "            x_current, y_current = x[i], y[i]\n",
    "            if sent==prev_sent:\n",
    "                prev_x.append(x_current)\n",
    "                prev_y.append(y_current)                                   \n",
    "            else:\n",
    "                if i!=0:\n",
    "                    train_sents2.append(prev_sent)\n",
    "                    train_spikes2.append(train_sents_tmp[i-1])\n",
    "                    x_new.append(np.mean(np.array(prev_x)))\n",
    "                    y_new.append(np.mean(np.array(prev_y)))\n",
    "                prev_x, prev_y = [x_current], [y_current]\n",
    "                prev_sent = sent \n",
    "            if i==len(train_sents_tmp):\n",
    "                train_sents2.append(sent)\n",
    "                train_spikes2.append(train_sents_tmp[i])\n",
    "                x_new.append(np.mean(np.array(prev_x)))\n",
    "                y_new.append(np.mean(np.array(prev_y)))             \n",
    "                \n",
    "        train_locs2 = pd.DataFrame(data=[x_new, y_new]).T\n",
    "        train_locs2.columns = ['x', 'y']\n",
    "    else:\n",
    "        train_locs2 = train_locs_tmp\n",
    "        train_sents2 = train_sents_tmp\n",
    "        train_spikes2 = train_spikes_tmp\n",
    "    \n",
    "    # Exclude remaining empty sentences from train:\n",
    "    x_new, y_new, train_sents3, train_spikes3 = [], [], [], []\n",
    "    x, y = train_locs2['x'].tolist(), train_locs2['y'].tolist()\n",
    "    for i, sent in enumerate(train_sents2):\n",
    "        if len(sent)!=0:\n",
    "            train_sents3.append(sent)\n",
    "            train_spikes3.append(train_spikes2[i])\n",
    "            x_new.append(x[i])\n",
    "            y_new.append(y[i])\n",
    "    \n",
    "    train_locs3 = pd.DataFrame(data=[x_new, y_new]).T\n",
    "    train_locs3.columns = ['x', 'y']\n",
    "    \n",
    "    \n",
    "    return [test_locs, test_locs_empty, test_sents, train_locs3, train_sents3, \n",
    "            train_spikes3, test_spikes, avg_loc, dists_df]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30276a09",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "056ad42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_callback(CallbackAny2Vec): # to print loss after each epoch\n",
    "    def __init__(self, model, epochs, model_path, s): \n",
    "        self.epoch = 1\n",
    "        self.tot_epochs = epochs\n",
    "        self.loss_previous_step=0\n",
    "        self.model_path = model_path\n",
    "        self.best_model = model\n",
    "        self.best_epoch = 1\n",
    "        self.best_loss = 1000000\n",
    "        self.s = s\n",
    "        \n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 1:\n",
    "            print(\"Training word2vec for \" + str(self.tot_epochs) + \" epochs...\")\n",
    "            current_loss = loss\n",
    "        else:\n",
    "            current_loss = loss-self.loss_previous_step # loss is cumulative\n",
    "            \n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "            self.best_model  = model\n",
    "            self.best_epoch = self.epoch\n",
    "            # print(\"Epoch \" + str(self.epoch)+ \": loss: \" + str(round(current_loss, 6)))\n",
    "            \n",
    "        # Last epoch:\n",
    "        if self.epoch == self.tot_epochs:\n",
    "            self.best_model.save(self.model_path + self.s + \"word2vec.model\")  \n",
    "            print(\"word2vec best epoch: \" +  str(self.best_epoch) + \", loss: \"+ str(round(self.best_loss, 6)))\n",
    "            \n",
    "        self.epoch+= 1\n",
    "        self.loss_previous_step = loss  \n",
    "         \n",
    "\n",
    "def make_wordvec_model(train_sents, vec_len, window_size, skipgram, batch_size, epochs, s, path_to_w2v):\n",
    "\n",
    "    model = Word2Vec(min_count=1, vector_size=vec_len, window=window_size, max_vocab_size=None,\n",
    "                     max_final_vocab=None, sg = skipgram,  compute_loss= True, batch_words=batch_size)\n",
    "\n",
    "    # Build vocabulary:\n",
    "    model.build_vocab(train_sents)\n",
    "    # a = model.wv.key_to_index\n",
    "    sent_counts = len(train_sents)\n",
    "    \n",
    "    # Train with callback:\n",
    "    model.train(corpus_iterable = train_sents, total_examples=sent_counts,\n",
    "                epochs=epochs, compute_loss=True, callbacks=[my_callback(model, epochs, path_to_w2v, s)]) \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "facb97b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For standardizing the input (needed especially for LSTM, but also important \n",
    "#for logistic regression; Random Forest doesn't care much):\n",
    "def standardize_col(col_train, col_test):\n",
    "    m = np.mean(col_train)\n",
    "    stdev = np.std(col_train)\n",
    "    col2_train = (col_train-m)/stdev\n",
    "    col2_test = (col_test-m)/stdev   \n",
    "    return [col2_train, col2_test]\n",
    "\n",
    "\n",
    "def make_vecs(model, neurons, train_sents, test_sents, train_spikes, test_spikes, \n",
    "              train_locs, test_locs, dists_df, use_weights):      \n",
    "\n",
    "    weight_col = dists_df['w+2*w^3'] # 'w+2*w^3', '5*(w+w^3)' # 'w+w^2+w^3'  'w+w^3' #  \n",
    "    #  '3*(w+w^3)' # '5*(w+w^3)'  # 'w+5*w^3' # 'w+3*w^3' #'7*(w+w^3)' #  \n",
    "    # '10*(w+w^3)' # '0.5*w+2*w^3',\n",
    "     \n",
    "    #-------------------------------------------------    \n",
    "    # Sentences:\n",
    "    \n",
    "    # Average vectors of each sentence:\n",
    "    train_vecs, test_vecs = [], [] # both train and test\n",
    "\n",
    "    weight_col.index = weight_col.index.astype('str')\n",
    "    for sent in train_sents:\n",
    "        # sent = train_sents[0]\n",
    "        vecs = [model.wv[code] for code in sent]\n",
    "        \n",
    "        # simple average:\n",
    "        if use_weights==False: # simple average\n",
    "            train_vecs.append(np.mean(np.array(vecs), axis = 0)) \n",
    "            \n",
    "        # weighted average:\n",
    "        else: \n",
    "            weights = [weight_col.loc[sent[x]] for x in range(len(sent))]\n",
    "            vecs = vecs*np.array(weights).reshape(len(weights),1)\n",
    "            train_vecs.append(np.mean(vecs, axis=0))\n",
    "            \n",
    "    for sent in test_sents:\n",
    "        vecs = [model.wv[code] for code in sent]\n",
    "        \n",
    "        # simple average:\n",
    "        if use_weights==False: # simple average\n",
    "            test_vecs.append(np.mean(np.array(vecs), axis = 0)) \n",
    "            \n",
    "        # weighted average:\n",
    "        else: \n",
    "            weights = [weight_col.loc[sent[x]] for x in range(len(sent))]\n",
    "            vecs = vecs*np.array(weights).reshape(len(weights),1)\n",
    "            test_vecs.append(np.mean(vecs, axis=0))\n",
    "        \n",
    "    df_train = pd.DataFrame(data=train_vecs)\n",
    "    df_test = pd.DataFrame(data=test_vecs)\n",
    "\n",
    "    # Normalize:\n",
    "    for col in df_train.columns.tolist():\n",
    "        train_col, test_col = standardize_col(df_train[col], df_test[col])\n",
    "        df_train[col] = train_col\n",
    "        df_test[col] = test_col \n",
    "\n",
    "    #-------------------------------------------------    \n",
    "    # Locations:\n",
    "    train_y = train_locs\n",
    "    test_y = test_locs\n",
    "\n",
    "    #-------------------------------------------------\n",
    "    \n",
    "    # Spikes:\n",
    "    df_train_spikes = pd.DataFrame(data=train_spikes)\n",
    "    df_test_spikes = pd.DataFrame(data=test_spikes)\n",
    "\n",
    "    # Not sure if multiplying each feature by some weight gives any effect...\n",
    "    if use_weights==True:\n",
    "        df_train_spikes = np.true_divide(df_train_spikes, weight_col.tolist()) \n",
    "        df_test_spikes = np.true_divide(df_test_spikes, weight_col.tolist()) \n",
    "    \n",
    "    # Normalize:\n",
    "    for col in df_train_spikes.columns.tolist():\n",
    "        train_col, test_col = standardize_col(df_train_spikes[col], df_test_spikes[col])\n",
    "        df_train_spikes[col] = train_col\n",
    "        df_test_spikes[col] = test_col \n",
    "    \n",
    "    \n",
    "    return [train_vecs, test_vecs, df_train, df_test, train_y, test_y, df_train_spikes, df_test_spikes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a02c3f",
   "metadata": {},
   "source": [
    "## Model fitting and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5516a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(reg_model, df_train2, train_y3, df_test2, test_y3, avg_loc, test_locs_empty, test_sents, lstm_seqlen):\n",
    "        \n",
    "     # \"<class 'tensorflow.python.keras.engine.functional.Functional'>\"\n",
    "    if \"tensorflow\" in str(type(reg_model)):  # Reshape test\n",
    "        test_features = np.array(df_test2)\n",
    "        df_test2, test_y3 = sliding_window(test_features, test_y3, lstm_seqlen)\n",
    "        preds = reg_model.predict(df_test2)\n",
    "    else:     \n",
    "        reg_model = reg_model.fit(df_train2, train_y3)\n",
    "        preds = reg_model.predict(df_test2)\n",
    "    \n",
    "    # Predictions (also add back intervals with no spikes, for which we always predict average location of rat in train set):\n",
    "    preds_df = pd.DataFrame(preds)\n",
    "    preds_avg_x = [avg_loc[0] for x in range(len(test_locs_empty))]\n",
    "    preds_avg_y = [avg_loc[1] for x in range(len(test_locs_empty))]\n",
    "    empty_preds_df = pd.DataFrame(data = [preds_avg_x, preds_avg_y]).T\n",
    "    preds_df = preds_df.append(empty_preds_df, ignore_index=True, sort = False) \n",
    "    preds_df.columns=['x', 'y']\n",
    "    \n",
    "    # Actuals for emptys:\n",
    "    emptys_count = len(test_locs_empty)\n",
    "    if emptys_count!=0:\n",
    "        test_y3 = test_y3.append(test_locs_empty, ignore_index=True, sort = False)\n",
    "        \n",
    "    if len(test_sents)!=0:  # only for sentences (not spikes)\n",
    "        test_sents2 = deepcopy(test_sents)\n",
    "        if \"tensorflow\" in str(type(reg_model)):\n",
    "            test_sents2 = test_sents2[seqlen-1:]\n",
    "        if emptys_count!=0:\n",
    "            for i in range(emptys_count):\n",
    "                test_sents2.append([])\n",
    "\n",
    "    \n",
    "    # Distance between predicted and actual location:\n",
    "    tmp_preds = preds_df.reset_index(drop=True)\n",
    "    tmp_test_y = test_y3.reset_index(drop=True)\n",
    "    dists = np.sqrt((tmp_test_y['x'] - tmp_preds['x'])**2 + (tmp_test_y['y'] -tmp_preds['y'])**2)\n",
    "    avg_dist_te = np.mean(dists)   \n",
    "    # print('test:', avg_dist_te, end = ', ')\n",
    "    median_dist_te = np.median(dists)\n",
    "    \n",
    "    \n",
    "    # Predictions for train: (doesn't include possible empty rows)\n",
    "    if \"tensorflow\" in str(type(reg_model)):  # Reshape test\n",
    "        tr_features = np.array(df_train2)\n",
    "        df_train2, train_y3 = sliding_window(tr_features, train_y3, lstm_seqlen)\n",
    "        \n",
    "    preds2 = reg_model.predict(df_train2)\n",
    "    preds_df2 = pd.DataFrame(preds2)\n",
    "    preds_df2.columns=['x', 'y']\n",
    "    tmp_preds2 = preds_df2.reset_index(drop=True)\n",
    "    tmp_train_y3 = train_y3.reset_index(drop=True)\n",
    "    \n",
    "    dists2 = np.sqrt((tmp_train_y3['x'] - tmp_preds2['x'])**2 + (tmp_train_y3['y'] - tmp_preds2['y'])**2)\n",
    "        \n",
    "    avg_dist_tr = np.mean(dists2)   \n",
    "    # print('train:', avg_dist_tr)\n",
    "    median_dist_tr = np.median(dists2)\n",
    "    \n",
    "    # Results as df:\n",
    "    df_preds = pd.DataFrame(data = [test_y3['x'].tolist(), test_y3['y'].tolist(), preds_df['x'].tolist(), preds_df['y'].tolist(), dists.tolist()]).T\n",
    "    df_preds.columns = ['x', 'y', 'pred_x', 'pred_y', 'dist']\n",
    "        \n",
    "    # Sentence lengths - only for sentences (not spikes):\n",
    "    if len(test_sents)!=0:\n",
    "        sent_lengths = [len(sent) for sent in test_sents2]\n",
    "        df_preds['sent']= test_sents2  \n",
    "        df_preds['length']= sent_lengths\n",
    "        df_preds = df_preds[['sent', 'length', 'x', 'y', 'pred_x', 'pred_y', 'dist']]\n",
    "    \n",
    "    return [df_preds, avg_dist_te, avg_dist_tr, median_dist_te, median_dist_tr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d148a",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3a12eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(X, y, seqlen):\n",
    "    # X - matrix where each row corresponds to a spike count vector of length nr_of_neurons\n",
    "    # y - rat positions at the center of those spike count windows\n",
    "    # seqlen - length of sequences we want to get out of this function\n",
    "\n",
    "    Xs = []\n",
    "    for i in range(seqlen): #0...99        \n",
    "        #100-99-1 >0\n",
    "        if seqlen - i - 1 > 0: #not the last piece\n",
    "            # take slices from 0 to -99, 1 to -98, ...,  98 to -1.\n",
    "            Xs.append(X[i:-(seqlen - i - 1), np.newaxis, ...])\n",
    "        else:  # cannot ask X[99:-0], so special case goes here\n",
    "            print(\"last piece to add\")\n",
    "            Xs.append(X[i:, np.newaxis, ...])\n",
    "\n",
    "    # we have seqlen(=100) slices each shifted in time. join them to get sequences of len 100\n",
    "    X = np.concatenate(Xs, axis=1)\n",
    "    y = y[seqlen - 1:]  # the positions are taken at the last timestep (from 99 to end)\n",
    "    print(f\"After sliding window: {(X.shape, y.shape)}\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def get_early_stopping_config(patience=0):\n",
    "  return keras.callbacks.EarlyStopping(patience=patience, verbose=1)\n",
    "\n",
    "def get_lr_config(lr=0.001, lr_factor=0.1, lr_epochs=None):\n",
    "  def lr_scheduler(epoch):\n",
    "    new_lr = lr * lr_factor ** int(epoch / lr_epochs)\n",
    "    print(f\"Epoch {epoch}: learning rate {new_lr}\")\n",
    "    return new_lr\n",
    "  return keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "def get_savepoints_config(filepath, save_best_model_only=False):\n",
    "  return keras.callbacks.ModelCheckpoint(filepath=filepath, verbose=1, save_best_model_only=save_best_model_only)\n",
    "\n",
    "def split_ndarray(df, train_ratio):\n",
    "  idx = math.floor(len(df) * train_ratio)\n",
    "  out = df[:idx], df[idx:]\n",
    "  return out\n",
    "\n",
    "def mse(y, t, axis=-1):\n",
    "    return np.square(y - t).mean(axis=axis).mean()\n",
    "\n",
    "def mean_distance(y, t, axis=-1):\n",
    "    return np.mean(np.sqrt(np.sum((y - t) ** 2, axis=axis)))\n",
    "\n",
    "def median_distance(y, t, axis=-1):\n",
    "    return np.median(np.sqrt(np.sum((y - t) ** 2, axis=axis)))\n",
    "\n",
    "\n",
    "def eval_model(model, X, y):  # enne: eval()\n",
    "  pred_y = model.predict(X)\n",
    "  if type(y) == pd.core.frame.DataFrame:\n",
    "    y = y.to_numpy()\n",
    "  err = mse(pred_y, y)\n",
    "  dist = mean_distance(pred_y, y)\n",
    "  median_dist = median_distance(pred_y, y)\n",
    "  return err, dist, median_dist\n",
    "\n",
    "def split_df(df, train_ratio):\n",
    "  idx = math.floor(len(df) * train_ratio)\n",
    "  return df[:idx], df[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55283273",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createLSTM(seqlen, num_features, dropout_ratio, hidden_nodes=512):\n",
    "    # hidden_nodes=1024 - Kristjanil: 1024 ; artiklis 512; ise proovides tundus 256 ok\n",
    "    num_outputs = 2\n",
    "\n",
    "    x = keras.layers.Input(shape=(seqlen, num_features), name=\"Input\")\n",
    "    h = x\n",
    "\n",
    "    # first LSTM layer with dropout\n",
    "    h = keras.layers.LSTM(hidden_nodes, input_shape=(seqlen, num_features), return_sequences=True, name=\"firstLstmBlock\")(h) # \n",
    "    h = keras.layers.Dropout(dropout_ratio, name=\"firstLstmDropout\")(h)\n",
    "\n",
    "    # second LSTM layer with dropout\n",
    "    h = keras.layers.LSTM(hidden_nodes, name=\"secondLstmBlock\")(h) \n",
    "    h = keras.layers.Dropout(dropout_ratio, name=\"secondLstmDropout\")(h)\n",
    "\n",
    "    # finally, add dense layer which acts as output layer\n",
    "    y = keras.layers.Dense(num_outputs, name=\"Output\")(h)\n",
    "\n",
    "    opt = keras.optimizers.RMSprop(learning_rate=0.001) # centered = True\n",
    "    #  no effect: momentum = 0.3 (default 0.0)\n",
    "    # https://keras.io/api/optimizers/rmsprop/\n",
    "    # optimizer \"rmsprop\": default learning rate: 0.001 (same as in article)\n",
    "\n",
    "    model = keras.models.Model(inputs=x, outputs=y, name=\"RatLSTM\")\n",
    "    model.compile(loss=mean_squared_error, optimizer=opt) # optimizer=\"rmsprop\"\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_lstm_nlp(input_features, locations, seqlen, train_ratio, epochs, dropout_ratio):\n",
    "  input_features = np.array(input_features)\n",
    "  train_feats, test_feats = split_ndarray(input_features, train_ratio)\n",
    "  train_y, test_y = split_df(locations, train_ratio)\n",
    "\n",
    "  train_X, train_y = sliding_window(train_feats, train_y, seqlen)\n",
    "  test_X, test_y = sliding_window(test_feats, test_y, seqlen)\n",
    "  # num_features=train_X.shape[2] # 125\n",
    "\n",
    "  model = createLSTM(seqlen=seqlen, num_features=train_X.shape[2], dropout_ratio=dropout_ratio)\n",
    "  \n",
    "  print(model.summary())\n",
    "  print(train_X.shape)\n",
    "  print(train_y.shape)\n",
    "  print(test_X.shape)\n",
    "  print(test_y.shape)\n",
    "\n",
    "  #callbacks = [WandbCallback()]\n",
    "  epochs = epochs\n",
    "  batch_size = 64  # 32\n",
    "  config = {\n",
    "    \"epochs\": epochs, \n",
    "    \"batch_size\": batch_size,\n",
    "    \"model-type\": \"original\",\n",
    "    \"data\": \"mean of word2vectors\",\n",
    "    \"seqlen\": seqlen,\n",
    "    \"train_X.shape\": train_X.shape,\n",
    "    \"train_y.shape\": train_y.shape,\n",
    "    \"test_X.shape\": test_X.shape,\n",
    "    \"test_y.shape\": test_y.shape,\n",
    "    \n",
    "  }\n",
    "  # wandb.init(project=\"LSTM\", entity=\"compneuro\", config=config)\n",
    "  history = model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs, validation_data=(test_X, test_y), shuffle=False) # callbacks=callbacks\n",
    "  \n",
    "  terr, tdist, tmediandist = eval_model(model, train_X, train_y)\n",
    "  verr, vdist, vmediandist = eval_model(model, test_X, test_y)\n",
    "  print('train mse = %g, validation mse = %g' % (terr, verr))\n",
    "  print('train mean dist = %g, validation mean dist = %g' % (tdist, vdist))\n",
    "  print('train median dist = %g, validation median dist = %g' % (tmediandist, vmediandist))\n",
    "  \n",
    "  return [model, history]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f3b3be",
   "metadata": {},
   "source": [
    "### Plot training losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa118acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history, epochs, seqlen, dropout):\n",
    "    name = \"ep=\" + str(epochs) + \", seqlen=\" + str(seqlen) + \", drop=\" + str(dropout) \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.ylim(0,500)\n",
    "    plt.title(name)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627c4c3",
   "metadata": {},
   "source": [
    "# Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88d6da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5410 steps:\n",
    "locations = read_data(path_to_data + \"R2192_1x200_at5_step200_bin100-RAW_pos.dat\")\n",
    "\n",
    "# 54100 steps:\n",
    "locations2 = pd.read_csv(path_to_data + \"R2192_20ms_speed_direction.csv\", encoding = \"UTF-8\") \n",
    "locations2 =locations2.drop(columns= ['speed', 'direction', 'direction_disp'])\n",
    "\n",
    "# rescale locations2 so that they are the same as locations (1mx1m):\n",
    "x_factor = locations.iloc[0]['x']/np.mean(locations2[:10]['x'])\n",
    "y_factor = locations.iloc[0]['y']/np.mean(locations2[:10]['y'])\n",
    "locations2['x'] = locations2['x']*x_factor\n",
    "locations2['y'] = locations2['y']*y_factor\n",
    "\n",
    "# Spikes:\n",
    "fn = \"R2192_20ms_63_neurons.csv\" # 63 rows, 54100 cols\n",
    "df =  pd.read_csv(path_to_data + fn, encoding = \"UTF-8\", header=None) \n",
    "df = df.T\n",
    "\n",
    "# Neuron ids:\n",
    "neurons = [str(x) for x in range(63)]\n",
    "df.columns = neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832c19a",
   "metadata": {},
   "source": [
    "**Input parameters for gathering sentences, making word vectors:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b090f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 60 #60 # 50      # # window_size = 10 -> 200ms, window_size = 60 -> 1200ms  \n",
    "step =  7 # 7 # 10 # 3 # 10\n",
    "do_shuffle= True # TRUE!  \n",
    "repetitions = True # TRUE!\n",
    "rm_duplicates = False # # FALSE! (doesn't make sence if do_shuffle=True)\n",
    "data = df #\n",
    "w2v_vec_len= 125 # 250\n",
    "w2v_win_size = 7 # 6 # 5 #10 #10 # 5\n",
    "w2v_skipgram=0\n",
    "w2v_batch_size= 500 #500 # 500 - ok with non-augmented train data\n",
    "w2v_epochs= 600 # 600- good with non-augmented train data\n",
    "use_weights = True # use weighted average instead of simple average when combining neuron vectors into sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e83104a",
   "metadata": {},
   "source": [
    "**Input parameters for LSTM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4897e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50 # in article: 50\n",
    "seqlen = 30  # in article: 100\n",
    "dropout = 0.5 # # in article: 0.5\n",
    "\n",
    "# note: in LSTM model definition (cf. above) hidden_nodes=512 is used (same as in article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066bcbe5",
   "metadata": {},
   "source": [
    "**Sentences, spikes and locations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25b38703",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_data = make_sents(df, locations2, window_size, step,  repetitions, do_shuffle)\n",
    "sents, locs2, spikes = window_data[0], window_data[1], window_data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed5299",
   "metadata": {},
   "source": [
    "## 10-fold cross-validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da1ea2",
   "metadata": {},
   "source": [
    "Here, we will make train/test splits simultaneously for spike counts data and word vectors data, in order to ensure that they are exactly comparable (same window size, shift in moveing the window etc.)\n",
    "\n",
    "After preparing train/test data for spike counts and word vectors for current fold, models are trained and evaluated on both types of data. We gather resuts of all folds, so that they can be later aggregated, saved and analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45288b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-fold cv rmse-s:\n",
    "rmses_lstm_spikes = []   #[[fold1_te, fold1_tr], [fold2_te, fold2_tr], ...]\n",
    "rmses_lstm_sent = []\n",
    "\n",
    "# Predictions for test:\n",
    "pred_lstm_spikes, pred_lstm_sents = [], []\n",
    " #[fold1_df_pred, fold2_df_pred ...] # not strictly needed, just interesting to compare what was predicted..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49522c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### KATSETUSED:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle = False)\n",
    "kfold  = kf.split(sents)  \n",
    "\n",
    "w2v_numjobs = 10\n",
    "\n",
    "for fold in tqdm(range(0,10)):\n",
    "\n",
    "    # Indexes of this fold:\n",
    "    train_index, test_index = next(kfold) # kfold is generator object\n",
    "    \n",
    "    print('\\nFOLD = ', fold, ':')\n",
    "    \n",
    "    # if fold!=9:   # for testing\n",
    "    #     continue\n",
    "    #if fold==1:\n",
    "    #    break\n",
    "    \n",
    "    #PREPARE DATA FOR THIS FOLD: \n",
    "        \n",
    "    print('preparing data...')\n",
    "    result2 = make_train_test(sents, locs2, spikes, train_index, test_index, data, locations2, window_size, step, repetitions, do_shuffle, rm_duplicates, fold)\n",
    "    test_locs, test_locs_empty, test_sents = result2[0], result2[1], result2[2]\n",
    "    train_locs, train_sents = result2[3], result2[4]\n",
    "    train_spikes, test_spikes = result2[5], result2[6]\n",
    "    avg_loc, dists_df = result2[7], result2[8]\n",
    "    print('Min spikes of neuron:', int(np.min(dists_df['spike_times'])))\n",
    "    \n",
    "    # Make word2vec model: ONLY SENTS:\n",
    "    model = make_wordvec_model(train_sents, w2v_vec_len, w2v_win_size, w2v_skipgram, w2v_batch_size, w2v_epochs, s, path_to_w2v) # last model (might not be best)\n",
    "    model = Word2Vec.load(path_to_w2v + s + \"word2vec.model\") # laod best model      \n",
    "    \n",
    "    # Make vectors (only for sentences), and prepare (weighted) train/test dfs (for sents and spikes):\n",
    "    result = make_vecs(model, neurons, train_sents, test_sents, train_spikes, test_spikes, train_locs, test_locs, dists_df, use_weights)\n",
    "    train_vecs, test_vecs = result[0], result[1]\n",
    "    df_train, df_test= result[2], result[3]\n",
    "    df_train_spikes, df_test_spikes= result[6], result[7]  # pmst: trainX, testX\n",
    "    train_y, test_y = result[4], result[5]\n",
    "    \n",
    "    # -------------------------------------------\n",
    "    \n",
    "    # MODEL FOR SENTENCES:\n",
    "    model, history = test_lstm_nlp(input_features=df_train, locations=train_y, seqlen=5,\n",
    "                                   train_ratio=0.9, epochs = 10, dropout_ratio=0.1)  \n",
    "    \n",
    "        \n",
    "    # -------------------------------------------\n",
    "    \n",
    "    # MODEL FOR SPIKES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03619f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = test_lstm_nlp(input_features=df_train, locations=train_y, \n",
    "                               seqlen=15, train_ratio=0.9, epochs = 10, dropout_ratio=0.3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161e4114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9089951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad1290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c466b8d6",
   "metadata": {},
   "source": [
    "**Predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e923b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen=10  # CHECK BEFORE RUNNING!\n",
    "\n",
    "df_preds, rmse_te, rmse_tr = get_predictions(model, df_train, train_y, df_test, test_y, avg_loc, test_locs_empty, test_sents, lstm_seqlen=seqlen)\n",
    "print('test: ', rmse_te, 'train:', rmse_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9dcaae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4036259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8598f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD =  0 :\n",
      "preparing data...\n",
      "Min spikes of neuron: 50\n",
      "Training word2vec for 600 epochs...\n",
      "word2vec best epoch: 395, loss: 32998.0\n",
      "\n",
      "RMSEs of models with spikes (fold= 0 ):\n",
      "Linear regression: test: 20.431549 , train: 21.189604\n",
      "KNN: test: 19.539411 , train: 14.456225\n",
      "Random Forest: test: 17.384762 , train: 10.421377\n",
      "\n",
      "RMSEs of models with sentences (fold= 0 ):\n",
      "Linear regression: test: 19.574086 , train: 17.893544\n",
      "KNN: test: 16.951463 , train: 13.310524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [03:22<30:23, 202.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: test: 16.064719 , train: 5.02583\n",
      "---------------------------------------------\n",
      "\n",
      "FOLD =  1 :\n",
      "preparing data...\n",
      "Min spikes of neuron: 40\n",
      "Training word2vec for 600 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [04:14<38:12, 254.69s/it]"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle = False)\n",
    "kfold  = kf.split(sents)  \n",
    "\n",
    "w2v_numjobs = 10\n",
    "\n",
    "for fold in tqdm(range(0,10)):\n",
    "\n",
    "    # Indexes of this fold:\n",
    "    train_index, test_index = next(kfold) # kfold is generator object\n",
    "    \n",
    "    print('\\nFOLD = ', fold, ':')\n",
    "    \n",
    "    # if fold!=9:\n",
    "    #     continue\n",
    "    #if fold==1:\n",
    "    #    break\n",
    "    \n",
    "    #PREPARE DATA FOR THIS FOLD: \n",
    "        \n",
    "    print('preparing data...')\n",
    "    result2 = make_train_test(sents, locs2, spikes, train_index, test_index, data, locations2, window_size, step, repetitions, do_shuffle, rm_duplicates, fold)\n",
    "    test_locs, test_locs_empty, test_sents = result2[0], result2[1], result2[2]\n",
    "    train_locs, train_sents = result2[3], result2[4]\n",
    "    train_spikes, test_spikes = result2[5], result2[6]\n",
    "    avg_loc, dists_df = result2[7], result2[8]\n",
    "    print('Min spikes of neuron:', int(np.min(dists_df['spike_times'])))\n",
    "    \n",
    "    # Make word2vec model: ONLY SENTS:\n",
    "    model = make_wordvec_model(train_sents, w2v_vec_len, w2v_win_size, w2v_skipgram, w2v_batch_size, w2v_epochs, s, path_to_w2v) # last model (might not be best)\n",
    "    model = Word2Vec.load(path_to_w2v + s + \"word2vec.model\") # laod best model      \n",
    "    \n",
    "    # Make vectors (only for sentences), and prepare (weighted) train/test dfs (for sents and spikes):\n",
    "    result = make_vecs(model, neurons, train_sents, test_sents, train_spikes, test_spikes, train_locs, test_locs, dists_df, use_weights)\n",
    "    train_vecs, test_vecs = result[0], result[1]\n",
    "    df_train, df_test= result[2], result[3]\n",
    "    df_train_spikes, df_test_spikes= result[6], result[7]  # pmst: trainX, testX\n",
    "    train_y, test_y = result[4], result[5]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    # REGRESSION MODELS:\n",
    "    \n",
    "    # SPIKES:\n",
    "    \n",
    "    print('\\nRMSEs of models with spikes (fold=', fold, '):')\n",
    "    # - Spikes, linear regression:\n",
    "    reg_model = MultiOutputRegressor(LinearRegression()) \n",
    "    df_preds, rmse_te, rmse_tr = get_predictions(reg_model, df_train_spikes, train_y, df_test_spikes, test_y, avg_loc, test_locs_empty, []) \n",
    "    rmses_linreg_spikes.append([rmse_te, rmse_tr])\n",
    "    pred_linreg_spikes.append(df_preds)\n",
    "    print('Linear regression: test:', round(rmse_te, 6), ', train:', round(rmse_tr, 6))\n",
    "\n",
    "    # - Spikes, knn:\n",
    "    reg_model = MultiOutputRegressor(KNeighborsRegressor(n_neighbors=30)) \n",
    "    df_preds, rmse_te, rmse_tr = get_predictions(reg_model, df_train_spikes, train_y, df_test_spikes, test_y, avg_loc, test_locs_empty, []) \n",
    "    rmses_knn_spikes.append([rmse_te, rmse_tr])\n",
    "    pred_knn_spikes.append(df_preds)\n",
    "    print('KNN: test:', round(rmse_te, 6), ', train:', round(rmse_tr, 6))\n",
    "        \n",
    "    # - Spikes, Random Forest:\n",
    "    reg_model = MultiOutputRegressor(RandomForestRegressor(min_samples_leaf=15, n_jobs=w2v_numjobs)) \n",
    "    df_preds, rmse_te, rmse_tr = get_predictions(reg_model, df_train_spikes, train_y, df_test_spikes, test_y, avg_loc, test_locs_empty, [])   \n",
    "    rmses_rf_spikes.append([rmse_te, rmse_tr])\n",
    "    pred_rf_spikes.append(df_preds)\n",
    "    print('Random Forest: test:', round(rmse_te, 6), ', train:', round(rmse_tr, 6))\n",
    "    \n",
    "    # ----------------------------------------------\n",
    "    \n",
    "    # SENTENCES:\n",
    "    \n",
    "    print('\\nRMSEs of models with sentences (fold=', fold, '):')\n",
    "    # - Sentences, linear regression:\n",
    "    reg_model = MultiOutputRegressor(LinearRegression())  # 20 \n",
    "    df_preds, rmse_te, rmse_tr = get_predictions(reg_model, df_train, train_y, df_test, test_y, avg_loc, test_locs_empty, test_sents)\n",
    "    rmses_linreg_sent.append([rmse_te, rmse_tr])\n",
    "    pred_linreg_sents.append(df_preds)\n",
    "    print('Linear regression: test:', round(rmse_te, 6), ', train:', round(rmse_tr, 6))\n",
    "        \n",
    "    # - Sentences, knn:    \n",
    "    reg_model = MultiOutputRegressor(KNeighborsRegressor(n_neighbors=30))  # 20 \n",
    "    df_preds, rmse_te, rmse_tr = get_predictions(reg_model, df_train, train_y, df_test, test_y, avg_loc, test_locs_empty, test_sents)\n",
    "    rmses_knn_sent.append([rmse_te, rmse_tr])\n",
    "    pred_knn_sents.append(df_preds)\n",
    "    print('KNN: test:', round(rmse_te, 6), ', train:', round(rmse_tr, 6))\n",
    "        \n",
    "    # - Sentences, Random Forest:     \n",
    "    reg_model = MultiOutputRegressor(RandomForestRegressor(min_samples_leaf=5, n_jobs=6)) \n",
    "    df_preds, rmse_te, rmse_tr = get_predictions(reg_model, df_train, train_y, df_test, test_y, avg_loc, test_locs_empty, test_sents)\n",
    "    rmses_rf_sent.append([rmse_te, rmse_tr])\n",
    "    pred_rf_sents.append(df_preds)\n",
    "    print('Random Forest: test:', round(rmse_te, 6), ', train:', round(rmse_tr, 6))\n",
    "    \n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a1044",
   "metadata": {},
   "source": [
    "## All results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c18a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses = [rmses_linreg_spikes, rmses_knn_spikes, rmses_rf_spikes, rmses_linreg_sent, rmses_knn_sent, rmses_rf_sent]\n",
    "cols = ['linreg_spikes', 'knn_spikes', 'rf_spikes', 'linreg_sents', 'knn_sents', 'rf_sents']\n",
    "df_rmse_te = pd.DataFrame()\n",
    "df_rmse_tr = pd.DataFrame()\n",
    "for i, model_rmses in enumerate(rmses):\n",
    "    model_rmses = pd.DataFrame(data = model_rmses)\n",
    "    df_rmse_te[cols[i]] = model_rmses[0]\n",
    "    df_rmse_tr[cols[i]] = model_rmses[1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cac6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bb9decd",
   "metadata": {},
   "source": [
    "Test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e9246",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse_te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0895abf3",
   "metadata": {},
   "source": [
    "Train results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc3576",
   "metadata": {},
   "source": [
    "## Average RMSE over 10 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e374c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_rmse_te) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48cf1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_rmse_tr) # train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af15bc",
   "metadata": {},
   "source": [
    "## Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940956bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://matplotlib.org/stable/gallery/statistics/boxplot_color.html\n",
    "\n",
    "\n",
    "labels = ['Linear reg.', 'KNN', 'Random Forest']\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))\n",
    "\n",
    "# Spike counts:\n",
    "bplot1 = ax1.boxplot(df_rmse_te[['linreg_spikes', 'knn_spikes', 'rf_spikes']],\n",
    "                     vert=True,  # vertical box alignment\n",
    "                     patch_artist=True,  # fill with color\n",
    "                     labels=labels)  # will be used to label x-ticks\n",
    "ax1.set_title('Models with spike counts')\n",
    "ax1.set_ylabel('MEAN ERROR')\n",
    "\n",
    "# Sentences:\n",
    "bplot2 = ax2.boxplot(df_rmse_te[['linreg_sents', 'knn_sents', 'rf_sents']],\n",
    "                     #notch=True,  # notch shape\n",
    "                     vert=True,  # vertical box alignment\n",
    "                     patch_artist=True,  # fill with color\n",
    "                     labels=labels)  # will be used to label x-ticks\n",
    "ax2.set_title('Models with w2v sentences')\n",
    "\n",
    "colors = ['#fdae61', '#abdda4', '#2b83ba']\n",
    "for bplot in (bplot1, bplot2):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "# Horizontal gridlines\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.yaxis.grid(True)\n",
    "    # ax.set_xlabel('Models with spike counts')\n",
    "    # ax.set_ylabel('Models with w2v sentences')\n",
    "    ax.set_ylim(12, 30)\n",
    "\n",
    "#plt.title('Mean errors over 10 folds')\n",
    "plt.ylabel('MEAN ERROR')  \n",
    "# plt.savefig(\"simple_models_rsmes.svg\")\n",
    "# plt.savefig(\"simple_models_rsmes.png\")\n",
    "# plt.savefig(\"simple_models_rsmes.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f269986",
   "metadata": {},
   "source": [
    "### Combine predictions for saving\n",
    "\n",
    "Combine all predictions of all models of each fold to dataframe (to make comparing them easier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f172e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_preds_df(cols, is_spikes, dfs_orig):\n",
    "    dfs_list = []\n",
    "    for i, models_preds in enumerate(dfs_orig): # model\n",
    "        for k in range(10): # fold\n",
    "            fold_preds = models_preds[k]\n",
    "            if i==0:\n",
    "                if is_spikes==True:\n",
    "                    df_preds_fold = fold_preds[['x', 'y']]\n",
    "                else:\n",
    "                    df_preds_fold = fold_preds[['sent', 'length', 'x', 'y']]\n",
    "            else:\n",
    "                df_preds_fold = dfs_list[k]\n",
    "            model_name = cols[i].split('_')[0]\n",
    "            df_preds_fold[model_name + \"_x\"] = fold_preds['pred_x']\n",
    "            df_preds_fold[model_name + \"_y\"] = fold_preds['pred_y']\n",
    "            df_preds_fold[model_name + \"_dist\"] = fold_preds['dist']  \n",
    "            if i==0:\n",
    "                dfs_list.append(df_preds_fold )\n",
    "            else:\n",
    "                dfs_list[k] = df_preds_fold      \n",
    "    return dfs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3a6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs_spikes = [pred_linreg_spikes, pred_knn_spikes, pred_rf_spikes]\n",
    "pred_dfs_sents =  [pred_linreg_sents, pred_knn_sents, pred_rf_sents]\n",
    "cols_spikes = ['linreg_spikes', 'knn_spikes', 'rf_spikes']\n",
    "cols_sents = ['linreg_sents', 'knn_sents', 'rf_sents']\n",
    "\n",
    "dfs_pred_spikes = make_single_preds_df(cols_spikes, True, pred_dfs_spikes)\n",
    "dfs_pred_sents = make_single_preds_df(cols_sents, False, pred_dfs_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e4085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: df of sentences models of fold 0:\n",
    "dfs_pred_sents[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87699738",
   "metadata": {},
   "source": [
    "**Save results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_results + \"dfs_preds_folds.plk\", \"wb\") as fout:\n",
    "    plk.dump([dfs_pred_spikes, dfs_pred_sents], fout) \n",
    "\n",
    "df_rmse_te.to_csv(path_to_results + \"rmses_test.csv\", sep=\",\", index=False, encoding=\"utf-8\") \n",
    "df_rmse_tr.to_csv(path_to_results + \"rmses_train.csv\", sep=\",\", index=False, encoding=\"utf-8\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c68423d",
   "metadata": {},
   "source": [
    "**Load results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rmse_te = pd.read_csv(path_to_results + \"rmses_test.csv\", sep=\",\", encoding = \"UTF-8\") \n",
    "# df_rmse_tr = pd.read_csv(path_to_results + \"rmses_train.csv\", sep=\",\", encoding = \"UTF-8\") \n",
    "\n",
    "# with open(path_to_results + \"dfs_preds_folds.plk\", \"rb\") as fin:\n",
    "#     loaded = plk.load(fin)\n",
    "#     dfs_pred_spikes =loaded[0] \n",
    "#     dfs_pred_sents = loaded[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9086a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baka38",
   "language": "python",
   "name": "baka38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
