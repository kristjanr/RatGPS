{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b16a21",
   "metadata": {},
   "source": [
    "# NeuroNLP - new experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91b52a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a926f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec  \n",
    "from gensim.models import KeyedVectors \n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import pickle as plk\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from wandb.keras import WandbCallback\n",
    "import wandb\n",
    "from tensorflow.keras.losses import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caaf9c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_linux = False\n",
    "\n",
    "# File separator:\n",
    "if is_linux:\n",
    "    s = \"/\"  # linux\n",
    "else:\n",
    "    s = \"\\\\\" # windows\n",
    "\n",
    "path_to_data = \"data\" + s\n",
    "#path_to_NLP = \"NLP\" + s\n",
    "path_to_w2v = \"word2vec\" + s\n",
    "path_to_results = \"NLP_results\" +s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578da924",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51dc4736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fname):\n",
    "    with open(fname, \"r\", encoding=\"UTF-8\") as f:\n",
    "        data = [line.rstrip().split(' ') for line in f.readlines()]\n",
    "    df = pd.DataFrame(data = data)\n",
    "    if len(df.columns) == 2:\n",
    "        df.columns = ['x', 'y']\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].astype('float32')\n",
    "    else:\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].astype('int64')        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8801437",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9cfcc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sents(dfx, locs, window_size, step, repetitions, do_shuffle): # window size - No of 20 ms intervals \n",
    "    sents = []\n",
    "    spikes = [] # for comparison\n",
    "    start, end = 0, window_size\n",
    "    empty_windows = 0\n",
    "    x_new, y_new = [], []\n",
    "    x, y = locs['x'], locs['y']\n",
    "    \n",
    "    while end < len(dfx):\n",
    "        \n",
    "        # Sentences: \n",
    "        rows = dfx.iloc[start:end] # rows in window\n",
    "        row = np.sum(rows) # all spikes of each neuron in window\n",
    "        sent_words = []\n",
    "        \n",
    "        if np.sum(row)==0:\n",
    "            empty_windows+=1\n",
    "            sents.append(sent_words)\n",
    "        else: # if there were any spikes at all\n",
    "            for j, spike_count in enumerate(row):\n",
    "                if spike_count!=0:\n",
    "                    if repetitions==True:                    \n",
    "                        sent_words+=[row.index[j] for x in range(spike_count)]                             \n",
    "                    else:\n",
    "                        sent_words.append(row.index[j])\n",
    "            if do_shuffle==True:\n",
    "                shuffle(sent_words)\n",
    "            sents.append(sent_words)         \n",
    "        \n",
    "        # Spikes:\n",
    "        spikes.append(row.tolist())    \n",
    "            \n",
    "        # Locations:\n",
    "        if window_size==1:\n",
    "            loc_x, loc_y = x[start], y[start]\n",
    "        elif window_size%2==0: # even number\n",
    "            loc_ind = int(start+(window_size)/2)\n",
    "            loc_ind2 =  loc_ind-1\n",
    "            loc_x = (x[loc_ind] + x[loc_ind2])/2\n",
    "            loc_y = (y[loc_ind] + y[loc_ind2])/2\n",
    "        else: # odd number\n",
    "            loc_ind = int(start+(window_size-1)/2)\n",
    "            loc_x, loc_y = x[loc_ind], y[loc_ind]\n",
    "        x_new.append(loc_x)\n",
    "        y_new.append(loc_y)\n",
    "        start+=step\n",
    "        end+=step\n",
    "     \n",
    "    locs2 = pd.DataFrame(data=[x_new, y_new]).T\n",
    "    locs2.columns = ['x', 'y']   \n",
    "    \n",
    "    return [sents, locs2, empty_windows, spikes] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d0020",
   "metadata": {},
   "source": [
    "## Analysis of goodness of windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6f2d06",
   "metadata": {},
   "source": [
    "How many empty and total sentences with different window size and step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "044dc5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_windows(dfx, locations2, window_sizes, steps, repetitions):\n",
    "    for i in range(len(steps)):\n",
    "        window_size = window_sizes[i]\n",
    "        step = steps[i]\n",
    "        \n",
    "        sents, locations3, empty_windows, spikes = make_sents(dfx, locations2, window_size=window_size, step=step, repetitions=repetitions, do_shuffle=False)\n",
    "        sent_lens = pd.Series([len(s) for s in sents]).value_counts()\n",
    "        print('\\nwindow_size=', window_size, '(', window_size*20,'ms)', 'step=', step, '(', step*20,'ms)')\n",
    "        print('empty windows: ', empty_windows, 'out of', len(sents), '(', round(100*empty_windows/(len(sents)), 2),'%), non-empty:', len(sents)-empty_windows)\n",
    "        print(sent_lens)\n",
    "        \n",
    "# window_sizes = [30, 30, 30]\n",
    "# steps = [5, 10, 15]\n",
    "# analyse_windows(df, locations2, window_sizes, steps, repetitions=False)\n",
    "\n",
    "# window_sizes = [30, 40, 50]\n",
    "# steps = [15, 20, 25]\n",
    "# analyse_windows(df, locations2, window_sizes, steps, repetitions=False)\n",
    "\n",
    "# window_sizes = [60, 70, 80]\n",
    "# steps = [30, 35, 40]\n",
    "# analyse_windows(df, locations2, window_sizes, steps, repetitions=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec448f",
   "metadata": {},
   "source": [
    "## Calculate weights for neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f859ca3",
   "metadata": {},
   "source": [
    "Weight depends on how disperse or compact is the neuron's receptive field. Neurons with disperse fields receive lower weight in sentence, and those with more compact receptive field receive larger weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c398c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weights(dfx, locations2x, avg_loc):\n",
    "    \n",
    "    # Calculate centroid of each receptive field:\n",
    "    neurons = dfx.columns.tolist()\n",
    "    df_all = pd.concat([dfx, locations2x], axis=1)\n",
    "    centroids_x, centroids_y = [], []\n",
    "    centroids_y= []\n",
    "    spike_times = []\n",
    "    for neuron in neurons:\n",
    "        d = df_all[df_all[neuron]>0]\n",
    "        # d = df_all[df_all['42']>0]\n",
    "        spike_times.append(len(d))\n",
    "        if len(d)==0:  # if neuron didn't spike, assume it's centroid to be the default average location\n",
    "            centroids_x.append(avg_loc[0])\n",
    "            centroids_y.append(avg_loc[1])\n",
    "        else:\n",
    "            d = d[[neuron, 'x', 'y']]\n",
    "            centroids_x.append(np.sum(d[neuron]*d['x'])/np.sum(d[neuron]))\n",
    "            centroids_y.append(np.sum(d[neuron]*d['y'])/np.sum(d[neuron]))\n",
    "    df_centroids = pd.DataFrame(data = [neurons, centroids_x, centroids_y, spike_times]).T\n",
    "    df_centroids.columns = ['neuron', 'x', 'y', 'spike_times']\n",
    "    df_centroids.index = neurons\n",
    "    \n",
    "    # Average distance of spike locations to centroid:\n",
    "    means = []\n",
    "    st_devs = []\n",
    "    for neuron in neurons:\n",
    "        d = df_all[df_all[neuron]>0]\n",
    "        if len(d)==0: \n",
    "            means.append(45.0) # to have the missing neuron have very low weight\n",
    "            st_devs.append(30.0)   # stdev is now not actually used...    \n",
    "        else:\n",
    "            d = d[[neuron, 'x', 'y']]\n",
    "            # centroid = df_centroids.iloc[int(neuron)]\n",
    "            centroid = df_centroids.loc[neuron]\n",
    "            dists = np.sqrt((d['x']-centroid['x'])**2+(d['y']-centroid['y'])**2)\n",
    "            avg_dist = np.mean(dists)  \n",
    "            stdev_dists = np.std(dists)\n",
    "            # plt.hist(dists, bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110])\n",
    "            # plt.xlim(0,110)\n",
    "            means.append(avg_dist)\n",
    "            st_devs.append(stdev_dists) \n",
    "    dists_df = pd.DataFrame(data = [means, st_devs, spike_times]).T\n",
    "    dists_df.columns = ['avg_dist', 'std_dist', 'spike_times']\n",
    "    # plt.plot(sorted(dists_df['avg_dist']))\n",
    "    \n",
    "    dists_df.index = pd.Series(neurons).astype('int64')\n",
    "    \n",
    "    # Scale the distances:\n",
    "    a = np.max(means)-means # inverse # 0- worst, 33.3- best\n",
    "    # plt.plot(sorted(a)) # \n",
    "    a2 = (a/np.max(a))+0.1  # to avoid zero \n",
    "    \n",
    "    # TODO: When assigning weights, take better into account how many times the neuron \n",
    "    # spiked - if it spiked only a few times, its weight should be lower (because then \n",
    "    # we have less confidence that in future it will spike in similar locations). \n",
    "    # - For now, just multiply weight by 0.9 if there were less than 10 spikes:\n",
    "    b = pd.Series(spike_times)<10\n",
    "    a2 = pd.Series(a2).where(-b, a2*0.9) \n",
    "\n",
    "    dists_df['weight'] = a2    \n",
    "    dists_df['weight^3'] = a2**3 # to give more weight to more compact receptive fields\n",
    "    dists_df['w+2*w^3']=dists_df['weight'] + dists_df['weight^3']*2 \n",
    "    \n",
    "    # plt.plot(sorted(dists_df['weight'] ))\n",
    "    # plt.plot(sorted(dists_df['weight^3'] ))    \n",
    "    # plt.plot(sorted(dists_df['w+2*w^3']))\n",
    "\n",
    "    return dists_df\n",
    "\n",
    "# dists_df = calc_weights(df, locations2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6c8424",
   "metadata": {},
   "source": [
    "## Train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ca2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test(sents, locs2, spikes, train_index, test_index, dfx, locations2, \n",
    "                    win_size, step, rep, do_shuffle, remove_duplicates, fold):\n",
    "        \n",
    "    test_locs_tmp = locs2.iloc[test_index]\n",
    "    train_locs_tmp = locs2.iloc[train_index]\n",
    "    test_sents_tmp = pd.Series(sents).iloc[test_index].tolist()\n",
    "    train_sents_tmp = pd.Series(sents).iloc[train_index].tolist()\n",
    "    test_spikes_tmp = pd.Series(spikes).iloc[test_index].tolist()\n",
    "    train_spikes_tmp =pd.Series(spikes).iloc[train_index].tolist()\n",
    "\n",
    "    # Average location in train set (for predicting empty rows in test set):\n",
    "    avg_loc = [np.mean(train_locs_tmp['x']), np.mean(train_locs_tmp['y'])]\n",
    "    \n",
    "    # Calculate weights for neurons, based on train set:\n",
    "    # -Those indexes are based on 1/10 parts of the original dataframe (the one \n",
    "    #containing the sentences is already shorter, because we aggregated data in each window.\n",
    "    split_size = int(len(dfx)/10)  # 5410\n",
    "    if fold==0:\n",
    "        start = split_size\n",
    "        end =  len(dfx)\n",
    "        dists_df = calc_weights(dfx.iloc[start:end], locations2.iloc[start:end], avg_loc)\n",
    "    elif fold==9:\n",
    "        start = 0\n",
    "        end = len(dfx) - split_size\n",
    "        dists_df = calc_weights(dfx.iloc[start:end], locations2.iloc[start:end], avg_loc)\n",
    "    else: \n",
    "        start = 0\n",
    "        end = fold*split_size\n",
    "        start2 = fold*split_size + split_size\n",
    "        end2 = len(dfx)\n",
    "        dfx_part1 = dfx.iloc[start:end]\n",
    "        dfx_part2 = dfx.iloc[start2:end2]\n",
    "        locs_part1 = locations2.iloc[start:end]\n",
    "        locs_part2 = locations2.iloc[start2:end2]    \n",
    "        dfx_parts_all = dfx_part1.append(dfx_part2, sort = False) \n",
    "        locs_parts_all = locs_part1.append(locs_part2, sort = False) \n",
    "        dists_df = calc_weights(dfx_parts_all, locs_parts_all, avg_loc)\n",
    "    \n",
    "    # Separate empty rows and their locations from test set:\n",
    "    # - (they will be put back before calculating RMSE)\n",
    "    x_new, y_new, test_sents, test_spikes = [], [], [], []\n",
    "    x_new_empty, y_new_empty = [], []\n",
    "    x, y = test_locs_tmp['x'], test_locs_tmp['y']\n",
    "    loc_ids = x.index.tolist()\n",
    "    for i, sent in enumerate(test_sents_tmp):\n",
    "        if len(sent)!=0:\n",
    "            test_sents.append(sent)\n",
    "            test_spikes.append(test_spikes_tmp[i])\n",
    "            x_new.append(x[loc_ids[i]])\n",
    "            y_new.append(y[loc_ids[i]])\n",
    "        else:\n",
    "            x_new_empty.append(x[loc_ids[i]])\n",
    "            y_new_empty.append(y[loc_ids[i]])\n",
    "    \n",
    "    test_locs_empty = pd.DataFrame(data=[x_new_empty, y_new_empty]).T\n",
    "    test_locs_empty.columns = ['x', 'y']\n",
    "    test_locs = pd.DataFrame(data=[x_new, y_new]).T\n",
    "    test_locs.columns = ['x', 'y']\n",
    "        \n",
    "\n",
    "    # If there is same sentence in consequtive windows, leave only one such sentence, \n",
    "    #and average the location (this occurs because of moving window: if 20ms intervals \n",
    "    #at the edges of the window were empty, same sentence was included several times)\n",
    "    #- ONLY MAKES SENCE IF \"DO_SHUFFLE\" = False. BUT ACTUALLY, SINCE WE SHUFFLE, THIS \n",
    "    #HAS BECOME USELESS...\n",
    "    if remove_duplicates==True:\n",
    "        x_new, y_new, train_sents2, train_spikes2 = [], [], [], []\n",
    "        x, y = train_locs_tmp['x'], train_locs_tmp['y']\n",
    "        prev_x, prev_y = [], []\n",
    "        prev_sent = ['']\n",
    "        for i, sent in enumerate(train_sents_tmp):\n",
    "            x_current, y_current = x[i], y[i]\n",
    "            if sent==prev_sent:\n",
    "                prev_x.append(x_current)\n",
    "                prev_y.append(y_current)                                   \n",
    "            else:\n",
    "                if i!=0:\n",
    "                    train_sents2.append(prev_sent)\n",
    "                    train_spikes2.append(train_sents_tmp[i-1])\n",
    "                    x_new.append(np.mean(np.array(prev_x)))\n",
    "                    y_new.append(np.mean(np.array(prev_y)))\n",
    "                prev_x, prev_y = [x_current], [y_current]\n",
    "                prev_sent = sent \n",
    "            if i==len(train_sents_tmp):\n",
    "                train_sents2.append(sent)\n",
    "                train_spikes2.append(train_sents_tmp[i])\n",
    "                x_new.append(np.mean(np.array(prev_x)))\n",
    "                y_new.append(np.mean(np.array(prev_y)))             \n",
    "                \n",
    "        train_locs2 = pd.DataFrame(data=[x_new, y_new]).T\n",
    "        train_locs2.columns = ['x', 'y']\n",
    "    else:\n",
    "        train_locs2 = train_locs_tmp\n",
    "        train_sents2 = train_sents_tmp\n",
    "        train_spikes2 = train_spikes_tmp\n",
    "    \n",
    "    # Exclude remaining empty sentences from train:\n",
    "    x_new, y_new, train_sents3, train_spikes3 = [], [], [], []\n",
    "    x, y = train_locs2['x'].tolist(), train_locs2['y'].tolist()\n",
    "    for i, sent in enumerate(train_sents2):\n",
    "        if len(sent)!=0:\n",
    "            train_sents3.append(sent)\n",
    "            train_spikes3.append(train_spikes2[i])\n",
    "            x_new.append(x[i])\n",
    "            y_new.append(y[i])\n",
    "    \n",
    "    train_locs3 = pd.DataFrame(data=[x_new, y_new]).T\n",
    "    train_locs3.columns = ['x', 'y']\n",
    "    \n",
    "    \n",
    "    return [test_locs, test_locs_empty, test_sents, train_locs3, train_sents3, \n",
    "            train_spikes3, test_spikes, avg_loc, dists_df]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b53d9",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e84cc685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_callback(CallbackAny2Vec): # to print loss after each epoch\n",
    "    def __init__(self, model, epochs, model_path, s): \n",
    "        self.epoch = 1\n",
    "        self.tot_epochs = epochs\n",
    "        self.loss_previous_step=0\n",
    "        self.model_path = model_path\n",
    "        self.best_model = model\n",
    "        self.best_epoch = 1\n",
    "        self.best_loss = 1000000\n",
    "        self.s = s\n",
    "        \n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 1:\n",
    "            print(\"Training word2vec for \" + str(self.tot_epochs) + \" epochs...\")\n",
    "            current_loss = loss\n",
    "        else:\n",
    "            current_loss = loss-self.loss_previous_step # loss is cumulative\n",
    "            \n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "            self.best_model  = model\n",
    "            self.best_epoch = self.epoch\n",
    "            # print(\"Epoch \" + str(self.epoch)+ \": loss: \" + str(round(current_loss, 6)))\n",
    "            \n",
    "        # Last epoch:\n",
    "        if self.epoch == self.tot_epochs:\n",
    "            self.best_model.save(self.model_path + self.s + \"word2vec.model\")  \n",
    "            print(\"word2vec best epoch: \" +  str(self.best_epoch) + \", loss: \"+ str(round(self.best_loss, 6)))\n",
    "            \n",
    "        self.epoch+= 1\n",
    "        self.loss_previous_step = loss  \n",
    "         \n",
    "\n",
    "def make_wordvec_model(train_sents, vec_len, window_size, skipgram, batch_size, epochs, s, path_to_w2v):\n",
    "\n",
    "    model = Word2Vec(min_count=1, vector_size=vec_len, window=window_size, max_vocab_size=None,\n",
    "                     max_final_vocab=None, sg = skipgram,  compute_loss= True, batch_words=batch_size)\n",
    "\n",
    "    # Build vocabulary:\n",
    "    model.build_vocab(train_sents)\n",
    "    # a = model.wv.key_to_index\n",
    "    sent_counts = len(train_sents)\n",
    "    \n",
    "    # Train with callback:\n",
    "    model.train(corpus_iterable = train_sents, total_examples=sent_counts,\n",
    "                epochs=epochs, compute_loss=True, callbacks=[my_callback(model, epochs, path_to_w2v, s)]) \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "270c4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For standardizing the input (needed especially for LSTM, but also important \n",
    "#for logistic regression; Random Forest doesn't care much):\n",
    "def standardize_col(col_train, col_test):\n",
    "    m = np.mean(col_train)\n",
    "    stdev = np.std(col_train)\n",
    "    col2_train = (col_train-m)/stdev\n",
    "    col2_test = (col_test-m)/stdev   \n",
    "    return [col2_train, col2_test]\n",
    "\n",
    "\n",
    "def make_vecs(model, neurons, train_sents, test_sents, train_spikes, test_spikes, \n",
    "              train_locs, test_locs, dists_df, use_weights):      \n",
    "\n",
    "    weight_col = dists_df['w+2*w^3'] # 'w+2*w^3', '5*(w+w^3)' # 'w+w^2+w^3'  'w+w^3' #  \n",
    "    #  '3*(w+w^3)' # '5*(w+w^3)'  # 'w+5*w^3' # 'w+3*w^3' #'7*(w+w^3)' #  \n",
    "    # '10*(w+w^3)' # '0.5*w+2*w^3',\n",
    "     \n",
    "    #-------------------------------------------------    \n",
    "    # Sentences:\n",
    "    \n",
    "    # Average vectors of each sentence:\n",
    "    train_vecs, test_vecs = [], [] # both train and test\n",
    "\n",
    "    weight_col.index = weight_col.index.astype('str')\n",
    "    for sent in train_sents:\n",
    "        # sent = train_sents[0]\n",
    "        vecs = [model.wv[code] for code in sent]\n",
    "        \n",
    "        # simple average:\n",
    "        if use_weights==False: # simple average\n",
    "            train_vecs.append(np.mean(np.array(vecs), axis = 0)) \n",
    "            \n",
    "        # weighted average:\n",
    "        else: \n",
    "            weights = [weight_col.loc[sent[x]] for x in range(len(sent))]\n",
    "            vecs = vecs*np.array(weights).reshape(len(weights),1)\n",
    "            train_vecs.append(np.mean(vecs, axis=0))\n",
    "            \n",
    "    for sent in test_sents:\n",
    "        vecs = [model.wv[code] for code in sent]\n",
    "        \n",
    "        # simple average:\n",
    "        if use_weights==False: # simple average\n",
    "            test_vecs.append(np.mean(np.array(vecs), axis = 0)) \n",
    "            \n",
    "        # weighted average:\n",
    "        else: \n",
    "            weights = [weight_col.loc[sent[x]] for x in range(len(sent))]\n",
    "            vecs = vecs*np.array(weights).reshape(len(weights),1)\n",
    "            test_vecs.append(np.mean(vecs, axis=0))\n",
    "        \n",
    "    df_train = pd.DataFrame(data=train_vecs)\n",
    "    df_test = pd.DataFrame(data=test_vecs)\n",
    "\n",
    "    # Normalize:\n",
    "    for col in df_train.columns.tolist():\n",
    "        train_col, test_col = standardize_col(df_train[col], df_test[col])\n",
    "        df_train[col] = train_col\n",
    "        df_test[col] = test_col \n",
    "\n",
    "    #-------------------------------------------------    \n",
    "    # Locations:\n",
    "    train_y = train_locs\n",
    "    test_y = test_locs\n",
    "\n",
    "    #-------------------------------------------------\n",
    "    \n",
    "    # Spikes:\n",
    "    df_train_spikes = pd.DataFrame(data=train_spikes)\n",
    "    df_test_spikes = pd.DataFrame(data=test_spikes)\n",
    "\n",
    "    # Not sure if multiplying each feature by some weight gives any effect...\n",
    "    if use_weights==True:\n",
    "        df_train_spikes = np.true_divide(df_train_spikes, weight_col.tolist()) \n",
    "        df_test_spikes = np.true_divide(df_test_spikes, weight_col.tolist()) \n",
    "    \n",
    "    # Normalize:\n",
    "    for col in df_train_spikes.columns.tolist():\n",
    "        train_col, test_col = standardize_col(df_train_spikes[col], df_test_spikes[col])\n",
    "        df_train_spikes[col] = train_col\n",
    "        df_test_spikes[col] = test_col \n",
    "    \n",
    "    \n",
    "    return [train_vecs, test_vecs, df_train, df_test, train_y, test_y, df_train_spikes, df_test_spikes]\n",
    "\n",
    "\n",
    "\n",
    "def get_predictions(reg_model, df_train2, train_y, df_test2, test_y, avg_loc, test_locs_empty, test_sents):\n",
    "    reg_model = reg_model.fit(df_train2, train_y)\n",
    "    \n",
    "    # Predictions (also add back intervals with no spikes, for which we always predict average \n",
    "    #location of rat in train set):\n",
    "    preds = reg_model.predict(df_test2)  \n",
    "    preds_df = pd.DataFrame(preds)\n",
    "    preds_avg_x = [avg_loc[0] for x in range(len(test_locs_empty))]\n",
    "    preds_avg_y = [avg_loc[1] for x in range(len(test_locs_empty))]\n",
    "    empty_preds_df = pd.DataFrame(data = [preds_avg_x, preds_avg_y]).T\n",
    "    preds_df = preds_df.append(empty_preds_df, ignore_index=True, sort = False) \n",
    "    preds_df.columns=['x', 'y']\n",
    "    \n",
    "    # Actuals for emptys:\n",
    "    emptys_count = len(test_locs_empty)\n",
    "    if emptys_count!=0:\n",
    "        test_y = test_y.append(test_locs_empty, ignore_index=True, sort = False)\n",
    "        \n",
    "    if len(test_sents)!=0:  # only for sentences (not spikes)\n",
    "        test_sents2 = deepcopy(test_sents)\n",
    "        if emptys_count!=0:\n",
    "            for i in range(emptys_count):\n",
    "                test_sents2.append([])\n",
    "\n",
    "    \n",
    "    # Distance between predicted and actual location:\n",
    "    dists = np.sqrt((test_y['x'] - preds_df['x'])**2 + (test_y['y'] -preds_df['y'])**2)\n",
    "    avg_dist_te = np.mean(dists)   \n",
    "    # print('test:', avg_dist_te, end = ', ')\n",
    "    \n",
    "    # Predictions for train: (doesn't include possible empty rows)\n",
    "    preds2 = reg_model.predict(df_train2)  \n",
    "    preds_df2 = pd.DataFrame(preds2)\n",
    "    preds_df2.columns=['x', 'y']\n",
    "    dists2 = np.sqrt((train_y['x'] - preds_df2['x'])**2 + (train_y['y'] -preds_df2['y'])**2)\n",
    "    avg_dist_tr = np.mean(dists2)   \n",
    "    # print('train:', avg_dist_tr)\n",
    "    \n",
    "    # Results as df:\n",
    "    df_preds = pd.DataFrame(data = [test_y['x'].tolist(), test_y['y'].tolist(), preds_df['x'].tolist(), \n",
    "                                    preds_df['y'].tolist(), dists.tolist()]).T\n",
    "    df_preds.columns = ['x', 'y', 'pred_x', 'pred_y', 'dist']\n",
    "        \n",
    "    # Sentence lengths - only for sentences (not spikes):\n",
    "    if len(test_sents)!=0:\n",
    "        sent_lengths = [len(sent) for sent in test_sents2]\n",
    "        df_preds['sent']= test_sents2  \n",
    "        df_preds['length']= sent_lengths\n",
    "        df_preds = df_preds[['sent', 'length', 'x', 'y', 'pred_x', 'pred_y', 'dist']]\n",
    "    \n",
    "    return [df_preds, avg_dist_te, avg_dist_tr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed8742",
   "metadata": {},
   "source": [
    "# Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "361a99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5410 steps:\n",
    "locations = read_data(path_to_data + \"R2192_1x200_at5_step200_bin100-RAW_pos.dat\")\n",
    "\n",
    "# 54100 steps:\n",
    "locations2 = pd.read_csv(path_to_data + \"R2192_20ms_speed_direction.csv\", encoding = \"UTF-8\") \n",
    "locations2 =locations2.drop(columns= ['speed', 'direction', 'direction_disp'])\n",
    "\n",
    "# rescale locations2 so that they are the same as locations (1mx1m):\n",
    "x_factor = locations.iloc[0]['x']/np.mean(locations2[:10]['x'])\n",
    "y_factor = locations.iloc[0]['y']/np.mean(locations2[:10]['y'])\n",
    "locations2['x'] = locations2['x']*x_factor\n",
    "locations2['y'] = locations2['y']*y_factor\n",
    "\n",
    "# Spikes:\n",
    "fn = \"R2192_20ms_63_neurons.csv\" # 63 rows, 54100 cols\n",
    "df =  pd.read_csv(path_to_data + fn, encoding = \"UTF-8\", header=None) \n",
    "df = df.T\n",
    "\n",
    "# Neuron ids:\n",
    "neurons = [str(x) for x in range(63)]\n",
    "df.columns = neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaa6673",
   "metadata": {},
   "source": [
    "**Input parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90273cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 60 #60 # 50      # # window_size = 10 -> 200ms, window_size = 60 -> 1200ms  \n",
    "step =  7 # 7 # 10 # 3 # 10\n",
    "do_shuffle= True # TRUE!  \n",
    "repetitions = True # TRUE!\n",
    "rm_duplicates = False # # FALSE! (doesn't make sence if do_shuffle=True)\n",
    "data = df #\n",
    "w2v_vec_len= 125 # 250\n",
    "w2v_win_size = 7 # 6 # 5 #10 #10 # 5\n",
    "w2v_skipgram=0\n",
    "w2v_batch_size= 500 #500 # 500 - ok with non-augmented train data\n",
    "w2v_epochs= 600 # 600- good with non-augmented train data\n",
    "use_weights = True # use weighted average instead of simple average when combining neuron vectors into sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9aa8a1",
   "metadata": {},
   "source": [
    "**Sentences, spikes and locations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7845b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_data = make_sents(df, locations2, window_size, step,  repetitions, do_shuffle)\n",
    "sents, locs2, spikes = window_data[0], window_data[1], window_data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c6891",
   "metadata": {},
   "source": [
    "## 10-fold cross-validation with simple models (linear regression, random forest and knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25790a68",
   "metadata": {},
   "source": [
    "Here, we will make train/test splits simultaneously for spike counts data and word vectors data, in order to ensure that they are exactly comparable (same window size, shift in moveing the window etc.)\n",
    "\n",
    "After preparing train/test data for spike counts and word vectors for current fold, three simple models are trained and evaluated on both types of data. We gather resuts of all folds, so that they can be later aggregated, saved and analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc9cd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-fold cv rmse-s:\n",
    "rmses_knn_spikes = [] #[[fold1_te, fold1_tr], [fold2_te, fold2_tr], ...]\n",
    "rmses_knn_sent = []\n",
    "rmses_linreg_spikes = []\n",
    "rmses_linreg_sent = []\n",
    "rmses_rf_spikes = []\n",
    "rmses_rf_sent = []\n",
    "\n",
    "# Predictions for test:\n",
    "pred_knn_spikes, pred_rf_spikes, pred_linreg_spikes = [], [], []\n",
    "pred_knn_sents, pred_rf_sents, pred_linreg_sents = [], [], []\n",
    " #[fold1_df_pred, fold2_df_pred ...] # not strictly needed, just interesting to compare what was predicted..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fc5a985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                      | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOLD =  0 :\n",
      "preparing data...\n",
      "Min spikes of neuron: 50\n",
      "Training word2vec for 600 epochs...\n",
      "word2vec best epoch: 502, loss: 34250.0\n",
      "\n",
      "RMSEs of models with spikes (fold= 0 ):\n",
      "Linear regression: test: 20.431549 , train: 21.189604\n",
      "KNN: test: 19.539865 , train: 14.456217\n",
      "Random Forest: test: 17.382597 , train: 10.462742\n",
      "\n",
      "RMSEs of models with sentences (fold= 0 ):\n",
      "Linear regression: test: 19.574364 , train: 17.894156\n",
      "KNN: test: 17.124585 , train: 13.340722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▌                                        | 1/10 [05:21<48:13, 321.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: test: 16.185198 , train: 5.031567\n",
      "---------------------------------------------\n",
      "\n",
      "FOLD =  1 :\n",
      "preparing data...\n",
      "Min spikes of neuron: 40\n",
      "Training word2vec for 600 epochs...\n",
      "word2vec best epoch: 565, loss: 32488.0\n",
      "\n",
      "RMSEs of models with spikes (fold= 1 ):\n",
      "Linear regression: test: 29.602144 , train: 20.185006\n",
      "KNN: test: 27.643017 , train: 13.89607\n",
      "Random Forest: test: 23.171943 , train: 10.147747\n",
      "\n",
      "RMSEs of models with sentences (fold= 1 ):\n",
      "Linear regression: test: 25.334749 , train: 17.180493\n",
      "KNN: test: 20.767453 , train: 12.790796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█████████                                    | 2/10 [09:59<39:28, 296.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: test: 21.422424 , train: 4.890472\n",
      "---------------------------------------------\n",
      "\n",
      "FOLD =  2 :\n",
      "preparing data...\n",
      "Min spikes of neuron: 51\n",
      "Training word2vec for 600 epochs...\n",
      "word2vec best epoch: 578, loss: 33730.0\n",
      "\n",
      "RMSEs of models with spikes (fold= 2 ):\n",
      "Linear regression: test: 23.152341 , train: 20.875644\n",
      "KNN: test: 21.941902 , train: 14.268738\n",
      "Random Forest: test: 18.807435 , train: 10.565072\n",
      "\n",
      "RMSEs of models with sentences (fold= 2 ):\n",
      "Linear regression: test: 19.347183 , train: 17.898317\n",
      "KNN: test: 15.515027 , train: 13.303761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████▌                               | 3/10 [15:17<35:42, 306.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: test: 15.192764 , train: 5.006104\n",
      "---------------------------------------------\n",
      "\n",
      "FOLD =  3 :\n",
      "preparing data...\n",
      "Min spikes of neuron: 53\n",
      "Training word2vec for 600 epochs...\n",
      "word2vec best epoch: 540, loss: 33388.0\n",
      "\n",
      "RMSEs of models with spikes (fold= 3 ):\n",
      "Linear regression: test: 18.77629 , train: 21.364541\n",
      "KNN: test: 16.755504 , train: 14.543038\n",
      "Random Forest: test: 15.416415 , train: 10.686651\n",
      "\n",
      "RMSEs of models with sentences (fold= 3 ):\n",
      "Linear regression: test: 18.474158 , train: 18.007219\n",
      "KNN: test: 16.518461 , train: 13.293221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████                           | 4/10 [20:44<31:24, 314.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: test: 15.608201 , train: 5.064911\n",
      "---------------------------------------------\n",
      "\n",
      "FOLD =  4 :\n",
      "preparing data...\n",
      "Min spikes of neuron: 52\n",
      "Training word2vec for 600 epochs...\n",
      "word2vec best epoch: 458, loss: 32910.0\n",
      "\n",
      "RMSEs of models with spikes (fold= 4 ):\n",
      "Linear regression: test: 23.906306 , train: 20.771098\n",
      "KNN: test: 18.928353 , train: 14.594459\n",
      "Random Forest: test: 17.764954 , train: 10.475315\n",
      "\n",
      "RMSEs of models with sentences (fold= 4 ):\n",
      "Linear regression: test: 20.496319 , train: 17.793561\n",
      "KNN: test: 17.252656 , train: 13.40274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████▌                      | 5/10 [26:04<26:21, 316.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: test: 16.898736 , train: 5.116307\n",
      "---------------------------------------------\n",
      "\n",
      "FOLD =  5 :\n",
      "preparing data...\n",
      "Min spikes of neuron: 54\n",
      "Training word2vec for 600 epochs...\n",
      "word2vec best epoch: 589, loss: 33674.0\n",
      "\n",
      "RMSEs of models with spikes (fold= 5 ):\n",
      "Linear regression: test: 21.518254 , train: 21.052473\n",
      "KNN: test: 17.575196 , train: 14.499132\n",
      "Random Forest: test: 14.378969 , train: 10.685602\n",
      "\n",
      "RMSEs of models with sentences (fold= 5 ):\n",
      "Linear regression: test: 16.940875 , train: 18.132548\n",
      "KNN: test: 14.704895 , train: 13.3963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████                  | 6/10 [31:29<21:17, 319.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: test: 13.276003 , train: 5.033866\n",
      "---------------------------------------------\n",
      "\n",
      "FOLD =  6 :\n",
      "preparing data...\n",
      "Min spikes of neuron: 52\n",
      "Training word2vec for 600 epochs...\n",
      "word2vec best epoch: 513, loss: 32494.0\n",
      "\n",
      "RMSEs of models with spikes (fold= 6 ):\n",
      "Linear regression: test: 23.27851 , train: 20.834402\n",
      "KNN: test: 18.845983 , train: 14.514744\n",
      "Random Forest: test: 15.909015 , train: 10.517533\n",
      "\n",
      "RMSEs of models with sentences (fold= 6 ):\n",
      "Linear regression: test: 19.357186 , train: 17.868674\n",
      "KNN: test: 16.024805 , train: 13.497019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████████████████████████████▍             | 7/10 [36:12<15:22, 307.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: test: 15.759884 , train: 5.126048\n",
      "---------------------------------------------\n",
      "\n",
      "FOLD =  7 :\n",
      "preparing data...\n",
      "Min spikes of neuron: 39\n",
      "Training word2vec for 600 epochs...\n",
      "word2vec best epoch: 556, loss: 32634.0\n",
      "\n",
      "RMSEs of models with spikes (fold= 7 ):\n",
      "Linear regression: test: 21.952731 , train: 20.98632\n",
      "KNN: test: 18.672924 , train: 14.355862\n",
      "Random Forest: test: 17.646732 , train: 10.561317\n",
      "\n",
      "RMSEs of models with sentences (fold= 7 ):\n",
      "Linear regression: test: 17.913279 , train: 18.049944\n",
      "KNN: test: 16.458605 , train: 13.399604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████         | 8/10 [40:54<09:58, 299.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: test: 15.924623 , train: 5.029823\n",
      "---------------------------------------------\n",
      "\n",
      "FOLD =  8 :\n",
      "preparing data...\n",
      "Min spikes of neuron: 49\n",
      "Training word2vec for 600 epochs...\n",
      "word2vec best epoch: 570, loss: 32076.0\n",
      "\n",
      "RMSEs of models with spikes (fold= 8 ):\n",
      "Linear regression: test: 26.974177 , train: 20.959255\n",
      "KNN: test: 24.236421 , train: 14.455861\n",
      "Random Forest: test: 21.2514 , train: 10.560544\n",
      "\n",
      "RMSEs of models with sentences (fold= 8 ):\n",
      "Linear regression: test: 23.168497 , train: 17.818518\n",
      "KNN: test: 21.559098 , train: 13.249714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████████▌    | 9/10 [45:59<05:01, 301.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: test: 19.137925 , train: 5.02258\n",
      "---------------------------------------------\n",
      "\n",
      "FOLD =  9 :\n",
      "preparing data...\n",
      "Min spikes of neuron: 33\n",
      "Training word2vec for 600 epochs...\n",
      "word2vec best epoch: 483, loss: 32596.0\n",
      "\n",
      "RMSEs of models with spikes (fold= 9 ):\n",
      "Linear regression: test: 22.749035 , train: 20.964748\n",
      "KNN: test: 21.493921 , train: 14.386808\n",
      "Random Forest: test: 19.664105 , train: 10.563522\n",
      "\n",
      "RMSEs of models with sentences (fold= 9 ):\n",
      "Linear regression: test: 18.72949 , train: 17.988179\n",
      "KNN: test: 17.813383 , train: 13.450544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 10/10 [48:51<00:00, 293.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: test: 16.396162 , train: 5.083835\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle = False)\n",
    "kfold  = kf.split(sents)  \n",
    "\n",
    "for fold in tqdm(range(0,10)):\n",
    "\n",
    "    # Indexes of this fold:\n",
    "    train_index, test_index = next(kfold) # kfold is generator object\n",
    "    \n",
    "    print('\\nFOLD = ', fold, ':')\n",
    "    \n",
    "    # if fold!=9:\n",
    "    #     continue\n",
    "    #if fold==1:\n",
    "    #    break\n",
    "    \n",
    "    #PREPARE DATA FOR THIS FOLD: \n",
    "        \n",
    "    print('preparing data...')\n",
    "    result2 = make_train_test(sents, locs2, spikes, train_index, test_index, data, locations2, window_size, step, repetitions, do_shuffle, rm_duplicates, fold)\n",
    "    test_locs, test_locs_empty, test_sents = result2[0], result2[1], result2[2]\n",
    "    train_locs, train_sents = result2[3], result2[4]\n",
    "    train_spikes, test_spikes = result2[5], result2[6]\n",
    "    avg_loc, dists_df = result2[7], result2[8]\n",
    "    print('Min spikes of neuron:', int(np.min(dists_df['spike_times'])))\n",
    "    \n",
    "    # Make word2vec model: ONLY SENTS:\n",
    "    model = make_wordvec_model(train_sents, w2v_vec_len, w2v_win_size, w2v_skipgram, w2v_batch_size, w2v_epochs, s, path_to_w2v) # last model (might not be best)\n",
    "    model = Word2Vec.load(path_to_w2v + s + \"word2vec.model\") # laod best model      \n",
    "    \n",
    "    # Make vectors (only for sentences), and prepare (weighted) train/test dfs (for sents and spikes):\n",
    "    result = make_vecs(model, neurons, train_sents, test_sents, train_spikes, test_spikes, train_locs, test_locs, dists_df, use_weights)\n",
    "    train_vecs, test_vecs = result[0], result[1]\n",
    "    df_train, df_test= result[2], result[3]\n",
    "    df_train_spikes, df_test_spikes= result[6], result[7]  # pmst: trainX, testX\n",
    "    train_y, test_y = result[4], result[5]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    # REGRESSION MODELS:\n",
    "        \n",
    "    print('\\nRMSEs of models with spikes (fold=', fold, '):')\n",
    "    # - Spikes, linear regression:\n",
    "    reg_model = MultiOutputRegressor(LinearRegression()) \n",
    "    df_preds, rmse_te, rmse_tr = get_predictions(reg_model, df_train_spikes, train_y, df_test_spikes, test_y, avg_loc, test_locs_empty, []) \n",
    "    rmses_linreg_spikes.append([rmse_te, rmse_tr])\n",
    "    pred_linreg_spikes.append(df_preds)\n",
    "    print('Linear regression: test:', round(rmse_te, 6), ', train:', round(rmse_tr, 6))\n",
    "\n",
    "    # - Spikes, knn:\n",
    "    reg_model = MultiOutputRegressor(KNeighborsRegressor(n_neighbors=30)) \n",
    "    df_preds, rmse_te, rmse_tr = get_predictions(reg_model, df_train_spikes, train_y, df_test_spikes, test_y, avg_loc, test_locs_empty, []) \n",
    "    rmses_knn_spikes.append([rmse_te, rmse_tr])\n",
    "    pred_knn_spikes.append(df_preds)\n",
    "    print('KNN: test:', round(rmse_te, 6), ', train:', round(rmse_tr, 6))\n",
    "        \n",
    "    # - Spikes, Random Forest:\n",
    "    reg_model = MultiOutputRegressor(RandomForestRegressor(min_samples_leaf=15, n_jobs=6)) \n",
    "    df_preds, rmse_te, rmse_tr = get_predictions(reg_model, df_train_spikes, train_y, df_test_spikes, test_y, avg_loc, test_locs_empty, [])   \n",
    "    rmses_rf_spikes.append([rmse_te, rmse_tr])\n",
    "    pred_rf_spikes.append(df_preds)\n",
    "    print('Random Forest: test:', round(rmse_te, 6), ', train:', round(rmse_tr, 6))\n",
    "    \n",
    "    # ----------------------------------------------\n",
    "    \n",
    "    print('\\nRMSEs of models with sentences (fold=', fold, '):')\n",
    "    # - Sentences, linear regression:\n",
    "    reg_model = MultiOutputRegressor(LinearRegression())  # 20 \n",
    "    df_preds, rmse_te, rmse_tr = get_predictions(reg_model, df_train, train_y, df_test, test_y, avg_loc, test_locs_empty, test_sents)\n",
    "    rmses_linreg_sent.append([rmse_te, rmse_tr])\n",
    "    pred_linreg_sents.append(df_preds)\n",
    "    print('Linear regression: test:', round(rmse_te, 6), ', train:', round(rmse_tr, 6))\n",
    "        \n",
    "    # - Sentences, knn:    \n",
    "    reg_model = MultiOutputRegressor(KNeighborsRegressor(n_neighbors=30))  # 20 \n",
    "    df_preds, rmse_te, rmse_tr = get_predictions(reg_model, df_train, train_y, df_test, test_y, avg_loc, test_locs_empty, test_sents)\n",
    "    rmses_knn_sent.append([rmse_te, rmse_tr])\n",
    "    pred_knn_sents.append(df_preds)\n",
    "    print('KNN: test:', round(rmse_te, 6), ', train:', round(rmse_tr, 6))\n",
    "        \n",
    "    # - Sentences, Random Forest:     \n",
    "    reg_model = MultiOutputRegressor(RandomForestRegressor(min_samples_leaf=5, n_jobs=6)) \n",
    "    df_preds, rmse_te, rmse_tr = get_predictions(reg_model, df_train, train_y, df_test, test_y, avg_loc, test_locs_empty, test_sents)\n",
    "    rmses_rf_sent.append([rmse_te, rmse_tr])\n",
    "    pred_rf_sents.append(df_preds)\n",
    "    print('Random Forest: test:', round(rmse_te, 6), ', train:', round(rmse_tr, 6))\n",
    "    \n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ce3d1",
   "metadata": {},
   "source": [
    "### All results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec606866",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses = [rmses_linreg_spikes, rmses_knn_spikes, rmses_rf_spikes, rmses_linreg_sent, rmses_knn_sent, rmses_rf_sent]\n",
    "cols = ['linreg_spikes', 'knn_spikes', 'rf_spikes', 'linreg_sents', 'knn_sents', 'rf_sents']\n",
    "df_rmse_te = pd.DataFrame()\n",
    "df_rmse_tr = pd.DataFrame()\n",
    "for i, model_rmses in enumerate(rmses):\n",
    "    model_rmses = pd.DataFrame(data = model_rmses)\n",
    "    df_rmse_te[cols[i]] = model_rmses[0]\n",
    "    df_rmse_tr[cols[i]] = model_rmses[1]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dde8a8",
   "metadata": {},
   "source": [
    "Test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "047bdd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linreg_spikes</th>\n",
       "      <th>knn_spikes</th>\n",
       "      <th>rf_spikes</th>\n",
       "      <th>linreg_sents</th>\n",
       "      <th>knn_sents</th>\n",
       "      <th>rf_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.431549</td>\n",
       "      <td>19.539865</td>\n",
       "      <td>17.382597</td>\n",
       "      <td>19.574364</td>\n",
       "      <td>17.124585</td>\n",
       "      <td>16.185198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.602144</td>\n",
       "      <td>27.643017</td>\n",
       "      <td>23.171943</td>\n",
       "      <td>25.334749</td>\n",
       "      <td>20.767453</td>\n",
       "      <td>21.422424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.152341</td>\n",
       "      <td>21.941902</td>\n",
       "      <td>18.807435</td>\n",
       "      <td>19.347183</td>\n",
       "      <td>15.515027</td>\n",
       "      <td>15.192764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.776290</td>\n",
       "      <td>16.755504</td>\n",
       "      <td>15.416415</td>\n",
       "      <td>18.474158</td>\n",
       "      <td>16.518461</td>\n",
       "      <td>15.608201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.906306</td>\n",
       "      <td>18.928353</td>\n",
       "      <td>17.764954</td>\n",
       "      <td>20.496319</td>\n",
       "      <td>17.252656</td>\n",
       "      <td>16.898736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.518254</td>\n",
       "      <td>17.575196</td>\n",
       "      <td>14.378969</td>\n",
       "      <td>16.940875</td>\n",
       "      <td>14.704895</td>\n",
       "      <td>13.276003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.278510</td>\n",
       "      <td>18.845983</td>\n",
       "      <td>15.909015</td>\n",
       "      <td>19.357186</td>\n",
       "      <td>16.024805</td>\n",
       "      <td>15.759884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21.952731</td>\n",
       "      <td>18.672924</td>\n",
       "      <td>17.646732</td>\n",
       "      <td>17.913279</td>\n",
       "      <td>16.458605</td>\n",
       "      <td>15.924623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26.974177</td>\n",
       "      <td>24.236421</td>\n",
       "      <td>21.251400</td>\n",
       "      <td>23.168497</td>\n",
       "      <td>21.559098</td>\n",
       "      <td>19.137925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22.749035</td>\n",
       "      <td>21.493921</td>\n",
       "      <td>19.664105</td>\n",
       "      <td>18.729490</td>\n",
       "      <td>17.813383</td>\n",
       "      <td>16.396162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   linreg_spikes  knn_spikes  rf_spikes  linreg_sents  knn_sents   rf_sents\n",
       "0      20.431549   19.539865  17.382597     19.574364  17.124585  16.185198\n",
       "1      29.602144   27.643017  23.171943     25.334749  20.767453  21.422424\n",
       "2      23.152341   21.941902  18.807435     19.347183  15.515027  15.192764\n",
       "3      18.776290   16.755504  15.416415     18.474158  16.518461  15.608201\n",
       "4      23.906306   18.928353  17.764954     20.496319  17.252656  16.898736\n",
       "5      21.518254   17.575196  14.378969     16.940875  14.704895  13.276003\n",
       "6      23.278510   18.845983  15.909015     19.357186  16.024805  15.759884\n",
       "7      21.952731   18.672924  17.646732     17.913279  16.458605  15.924623\n",
       "8      26.974177   24.236421  21.251400     23.168497  21.559098  19.137925\n",
       "9      22.749035   21.493921  19.664105     18.729490  17.813383  16.396162"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rmse_te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5571db8",
   "metadata": {},
   "source": [
    "Train results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "904a6894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linreg_spikes</th>\n",
       "      <th>knn_spikes</th>\n",
       "      <th>rf_spikes</th>\n",
       "      <th>linreg_sents</th>\n",
       "      <th>knn_sents</th>\n",
       "      <th>rf_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.189604</td>\n",
       "      <td>14.456217</td>\n",
       "      <td>10.462742</td>\n",
       "      <td>17.894156</td>\n",
       "      <td>13.340722</td>\n",
       "      <td>5.031567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.185006</td>\n",
       "      <td>13.896070</td>\n",
       "      <td>10.147747</td>\n",
       "      <td>17.180493</td>\n",
       "      <td>12.790796</td>\n",
       "      <td>4.890472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.875644</td>\n",
       "      <td>14.268738</td>\n",
       "      <td>10.565072</td>\n",
       "      <td>17.898317</td>\n",
       "      <td>13.303761</td>\n",
       "      <td>5.006104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.364541</td>\n",
       "      <td>14.543038</td>\n",
       "      <td>10.686651</td>\n",
       "      <td>18.007219</td>\n",
       "      <td>13.293221</td>\n",
       "      <td>5.064911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.771098</td>\n",
       "      <td>14.594459</td>\n",
       "      <td>10.475315</td>\n",
       "      <td>17.793561</td>\n",
       "      <td>13.402740</td>\n",
       "      <td>5.116307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.052473</td>\n",
       "      <td>14.499132</td>\n",
       "      <td>10.685602</td>\n",
       "      <td>18.132548</td>\n",
       "      <td>13.396300</td>\n",
       "      <td>5.033866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20.834402</td>\n",
       "      <td>14.514744</td>\n",
       "      <td>10.517533</td>\n",
       "      <td>17.868674</td>\n",
       "      <td>13.497019</td>\n",
       "      <td>5.126048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20.986320</td>\n",
       "      <td>14.355862</td>\n",
       "      <td>10.561317</td>\n",
       "      <td>18.049944</td>\n",
       "      <td>13.399604</td>\n",
       "      <td>5.029823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20.959255</td>\n",
       "      <td>14.455861</td>\n",
       "      <td>10.560544</td>\n",
       "      <td>17.818518</td>\n",
       "      <td>13.249714</td>\n",
       "      <td>5.022580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20.964748</td>\n",
       "      <td>14.386808</td>\n",
       "      <td>10.563522</td>\n",
       "      <td>17.988179</td>\n",
       "      <td>13.450544</td>\n",
       "      <td>5.083835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   linreg_spikes  knn_spikes  rf_spikes  linreg_sents  knn_sents  rf_sents\n",
       "0      21.189604   14.456217  10.462742     17.894156  13.340722  5.031567\n",
       "1      20.185006   13.896070  10.147747     17.180493  12.790796  4.890472\n",
       "2      20.875644   14.268738  10.565072     17.898317  13.303761  5.006104\n",
       "3      21.364541   14.543038  10.686651     18.007219  13.293221  5.064911\n",
       "4      20.771098   14.594459  10.475315     17.793561  13.402740  5.116307\n",
       "5      21.052473   14.499132  10.685602     18.132548  13.396300  5.033866\n",
       "6      20.834402   14.514744  10.517533     17.868674  13.497019  5.126048\n",
       "7      20.986320   14.355862  10.561317     18.049944  13.399604  5.029823\n",
       "8      20.959255   14.455861  10.560544     17.818518  13.249714  5.022580\n",
       "9      20.964748   14.386808  10.563522     17.988179  13.450544  5.083835"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rmse_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77575de8",
   "metadata": {},
   "source": [
    "### Average RMSE over 10 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcc2ff6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linreg_spikes    23.234134\n",
       "knn_spikes       20.563309\n",
       "rf_spikes        18.139357\n",
       "linreg_sents     19.933610\n",
       "knn_sents        17.373897\n",
       "rf_sents         16.580192\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df_rmse_te) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdb1fb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linreg_spikes    20.918309\n",
       "knn_spikes       14.397093\n",
       "rf_spikes        10.522604\n",
       "linreg_sents     17.863161\n",
       "knn_sents        13.312442\n",
       "rf_sents          5.040551\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df_rmse_tr) # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c698c352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MEAN ERROR')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAAEICAYAAABrgD+dAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAsMklEQVR4nO3de5xcdX3/8debGAEhAW0gQshFUWTLlh/W9VZWySpairf+qKhbtKBbo7WuoNBaGStQnRZveAn2Z/G3CKhM1R9IlVoE213oyM2EcgmOgiIkgQgiIkENBvj8/jjfJcMyO7uT7Jnr+/l4zCNnzuV7vnNm9pPP+Z7v+R5FBGZmZmbtYKdWV8DMzMxskhMTMzMzaxtOTMzMzKxtODExMzOztuHExMzMzNqGExMzMzNrG05M2pSkFZJC0pNmse5xkso51uVmSSvrLJ+Q9Jd57T/t4yWSflT1/nZJh+e5T7N25zhh3ciJyRxI/0n+TtKiKfOvT0FjRYuqNici4qCImACQdKqkL7egDv8dEc9p9n7zkH4Tz2p1Pay5HCfmnqRXSSpLul/SzyR9QdKCvPc7W3kng93Kicnc+SkwPPlG0h8Au7auOmbWhhwn5tYewEeAfYE+YD/g4y2tke0wJyZz50vAX1S9PxY4r3oFSXtIOk/SzyXdIemDknZKy+ZJ+oSkeyXdBryqxrZjkjZJulPSRyTNm1oJZT4l6R5Jv5J0o6T+GusNSbqp6v13JV1b9b4s6U/T9O2SDpd0BHAy8EZJD0q6oarI5ZK+J2mzpEunnhVWlbtI0sXpDOc+Sf9ddQxul/QBST+Q9EtJX5S0S1q2UtLGaco8UNJPJb0pvX91Ogu9X9KVkg6utV1a9yBJl6W63C3p5DR/Z0mflnRXen1a0s5p2RPOgqpbQSSdI+lzkv49HY9rJO2fll2RNrkhHcM31jsm1nUcJ2YXJy6X9GdpejD9fR2Z3h8u6XqAiDg/Ii6JiN9ExC+BLwCHpvXeJGnNlHLfK+mb0+zzOEm3pbr9VNIxVcveJqmS4tJ3JC2vWhaS3inp1rT8c+n49gGfB16cjsP9af2d03e4PsWcz0vaNS1bKWmjpBPTd7NJ0lur9rWrpE+m38Wv0vGf3PZFyuLd/ZJuUNVltXqfrS1FhF87+AJuBw4HfkSWtc8DNgDLgQBWpPXOA/4NWACsAG4BRtKydwI/BJYCTwPG07ZPSssvAv4F2A3YG7gWeEdadhxQTtN/DKwF9gSU6rNPjTrvAvwWWAQ8CfgZcFeq265p2e9Vf740fSrw5SllTQA/AQ5I204Ap09zrP6J7I91fnq9BFDVftZVHYPvAR9Jy1YCG2sc8z8E1gOvTvP/ELgHeGH6Ho5N6+5coy4LgE3Aiel4LABemJb9A3B1OtZ7AVcCH556vKvKCuBZafoc4D7gBenYfgX411rrznRM/OqeF44TE8w+TvwDsDpNn5y2+2jVss9Ms92nJ//WgKcAm4FnVy3/PvCmGtvtBjwAPCe93wc4KE3/KfDjdIyeBHwQuLJq2wAuTsdyGfBz4Iipx3xKHb+Zvr8FwLeAf0rLVgIPp884HzgS+A3w1LT8c+m4LSH7/fwRsHN6/4u0/k7AK9L7vep9tnZ9+axsbk2eDb2CLHjcObkgnbW8EfhARGyOiNuBTwJvSau8Afh0RGyIiPvI/rOa3HYx8CfACRHx64i4B/gU8KYaddhK9mM/kOw/t0pEbJq6UkRsAdYALwUGgBuBMtnZxouAWyPiFw189i9GxC0R8Vvga8Ah06y3lewPY3lEbI2s70j1A5vOrDoGRaqavWt4Cdkf+LERcXGa93bgXyLimoh4JCLOBR5Kn2mqVwM/i4hPRsSW9L1ck5YdA/xDRNwTET8HTmPbdzUbF0bEtRHxMFlickiddWc6JtZdHCdmjhOXA4el6Zemzzn5/rC0/HEkvYLsRORDqe6/IUvwhtPyZ6fPW7PFBHgU6Je0a0Rsioib0/x3kCUOlfT3/I/AIdWtJmQJ1v0RsZ4sWaz5uSSJLEa9NyLui4jNqbzq72grWezZGhHfBh4EnpNazd4GHB8Rd6b4dmVEPAS8Gfh2RHw7Ih6NiMvIvrcjZ/hsbcmJydz6EvDnZFnyeVOWLQKeDNxRNe8OskwXsmukG6Ysm7ScLHvelJrp7ic7K9p7agUi4r+AM8ky67slnSVp4TT1vZwsQ39pmp4g+6Ov+Yc/g59VTf8G2H2a9T5OdvZxaWpa/Lspy6ceg33r7POdZGcu41XzlgMnTh6ndKyWTlPOUrIzsVr25YnfVb26TDXb4wEzHxPrLo4TmXp/F1cBB6Rk6xCy47Q0Xfp5AXBF9cqSXgScD7w+Im6pWnQ+205u/hy4KCUsjxMRvyZLCN9Jdvz+XdKBafFy4DNVx/Q+slamJVVFzPZz7UXWkrO2qrxL0vxJv0gJ0NTyFpG1YNWKWcuBo6fEvUGyVrB6n60tOTGZQxFxB1nntiOBC6csvpcsE67Ospex7WxpE9l/lNXLJm0gO+tfFBF7ptfCiDhomnp8NiKeBxxE1mz6N9NUeWrAmTxLqRdwduhMPp0FnhgRzwReA7xP0surVpl6DO6qU9w7gWWSPlU1bwNQrDpOe0bEUyKiVGP7DcD+05R9F0/8ribr8muy4AKApKfXqeOMZnFMrIs4TswsJQ9rgeOBdRHxO7LLqe8DfhIR906uK+m5ZK0gb4uI/5xS1KXAIkmHkCUo59fZ53ci4hVkrZc/JOuvAtlxfceUmLJrRFw5m48y5f29ZJe/Dqoqa4+IqHfiUr3tFmrHrA3Al6bUcbeIOH2Gz9aWnJjMvRHgZSlLfUxEPELWdFmUtCA1A74PmLyl7mvAeyTtJ+mpwN9VbbuJ7A/sk5IWStpJ0v6SDmMKSc+X9EJJ88n+A90CPDJNXa8EnkN2BnJtat5bTtY/44pptrkbWKHt7JyprGPqs1KT5gOpbtX1++t0DJ5Gdm35q3WK2wwcAbxU0ulp3heAd6ZjIEm7KbulsNYthBcDT5d0QuqQtkDSC9OyEvBBSXuls7QPse27ugE4SNIhyjrnntrgYbgbeObkm1kcE+s+jhMzuxx4N9uSn4kp71HWYfcSYDQivjW1gNTy8P/IWiWfBlxWa0eSFkt6raTdyJK7B9l2PD4PfEDSQWndPSQdPcvPcDewn6Qnp/o8ShajPiVp71TeEkl/PFNBaduzgTMk7ausI/SLlXXK/zLwGkl/nObvoqwj7X4zfLa25MRkjkXETyJizTSLR8mCwG1k12nPJ/uhQfZj/Q7Zf3rX8cQzqb8ga+L9AfBLsj+2fWrsY2Eq65dkzby/AD4xTV1/nfZ1czojgawJ9Y50fbqWr6d/fyHpumnWqefZwHfJ/jiuAv450tgHyflkwfW29PpIvcIi4n6ya/V/IunD6di/nayZ+pdkl0iOm2bbzWnb15A1xd4KDKXFHyG7RnsjcBPZcfpI2u4Wss5p303bNDpOwanAuanJ9Q3MfEysyzhOzMrlZP1grpjmPWQd1/cCxpTd+fKgpKn9J84n63T89SmXSKrtlMq6i+xSzWHAuwAi4hvAR4F/lfQAWQf9P5nlZ/gv4GbgZ5ImW3neTxaXrk7lfZcs8ZuNk8ji0fdTPT8K7BQRG4DXkZ3M/ZysBeVv0uea9rO1q8m7IcxaTtLtwF9GxHdbXRczM2sNt5iYmZlZ28gtMUnXuK5VNtDLzZJOS/OfpmxAq1vTv0/Nqw5m1tkcR8x6T26XclJHvt0i4sHUwapM1sP6KOC+iDhd2W2RT42I9+dSCTPraI4jZr0ntxaTyDyY3k6OaBlkHXTOTfPPJRtVz8zsCRxHzHrPjI/K3hHKRjFcCzwL+FxEXCNpcbqtjYjYNHnLVI1tVwGrAHbdddfnLV26tNZqZtZkt9xyy70RsdfMa84NxxGz7lMvjjTlrhxJewLfILsNrhwRe1Yt+2VE1L0+PDAwEGvWTHdnnZk1k6S1ETHQgv3uieOIWVeoF0eacldOGmtigmwwrLsl7ZMqtg/ZA9c6SqlUor+/n3nz5tHf30+pVGtQUTObS90WR8ystjzvytkrneGg7LHMh5MNhftNsgctkf79t7zqkIdSqUShUGD16tVs2bKF1atXUygUnJyY5aBb44iZTS/PFpN9gHFJN5KNUndZZE+APR14haRbyUbdPL1OGW2nWCwyNjbG0NAQ8+fPZ2hoiLGxMYrFYqurZtaNujKOmNn0OmLk13a6Njxv3jy2bNnC/PnzH5u3detWdtllFx55pK0fP2A2J1rVx2RHtVMcMet1Le9j0k36+voolx//aJRyuUxfX1+LamRmZtY9nJg0qFAoMDIywvj4OFu3bmV8fJyRkREKhUKrq2ZmZtbxch3HpBsNDw8DMDo6SqVSoa+vj2Kx+Nh8MzMz235OTLbD8PCwExEzM7Mc+FKOmZmZtQ0nJmZmZtY2nJiYmZlZ23BiYmZmZm3DiYmZmZm1DScmZmZm1jacmJiZmVnbcGJiZmZmbcOJiZmZmbUNJyZmZmbWNpyYmJmZWdvILTGRtFTSuKSKpJslHZ/mHyLpaknXS1oj6QV51cHMOpvjiFnvyfMhfg8DJ0bEdZIWAGslXQZ8DDgtIv5D0pHp/coc62FmnctxxKzH5JaYRMQmYFOa3iypAiwBAliYVtsDuCuvOphZZ3McMes9ebaYPEbSCuC5wDXACcB3JH2C7FLSHzWjDmbW2RxHzHpD7omJpN2BC4ATIuIBSR8B3hsRF0h6AzAGHF5ju1XAKoDFixczMTGRd1XNrE05jpj1DkVEfoVL84GLge9ExBlp3q+APSMiJAn4VUQsrFfOwMBArFmzJrd6Wr5KpRLFYpFKpUJfXx+FQoHh4eFWV8u2k6S1ETHQxP05jph1mXpxJM+7ckR2FlOZDCbJXcBhafplwK151cFar1QqUSgUWL16NVu2bGH16tUUCgVKpVKrq2YdwHHErPfk1mIiaRD4b+Am4NE0+2TgAeAzZJeRtgDvioi19crymU7n6u/vZ/Xq1QwNDT02b3x8nNHRUdatW9fCmtn2amaLieOIWXeqF0fyvCunDGiaxc/La7/WXiqVCoODg4+bNzg4SKVSaVGNrJM4jpj1Ho/8arnq6+ujXC4/bl65XKavr69FNTIzs3bmxMRyVSgUGBkZYXx8nK1btzI+Ps7IyAiFQqHVVTMzszbUlHFMrHdN3n0zOjr62F05xWLRd+WYmVlNTkwsd8PDw05EzMxsVnwpZzuUSiX6+/uZN28e/f39vvXVzMxsjrjFpEGT43KMjY0xODhIuVxmZGQEwK0CZmZmO8gtJg0qFouMjY0xNDTE/PnzGRoaYmxsjGKx2OqqmZmZdTwnJg3yuBxmZmb5cWLSII/LYWZmlh8nJg3yuBxmZmb5cefXBnlcDjMzs/w4MdkOHpfDzMwsH76UY2ZmZm3DiYmZmZm1DScmZmZm1jacmJiZmVnbyC0xkbRU0rikiqSbJR1ftWxU0o/S/I/lVQcz62yOI2a9J8+7ch4GToyI6yQtANZKugxYDLwOODgiHpK0d451MLPO5jhi1mNyS0wiYhOwKU1vllQBlgBvB06PiIfSsnvyqoOZdTbHEbPe05RxTCStAJ4LXAN8HHiJpCKwBTgpIr5fY5tVwCqAxYsXMzEx0Yyqmlmbchwx6w25JyaSdgcuAE6IiAckPQl4KvAi4PnA1yQ9MyKieruIOAs4C2BgYCBWrlyZd1XNrE05jpj1jlzvypE0nyyYfCUiLkyzNwIXRuZa4FFgUZ71MLPO5Thi1lvyvCtHwBhQiYgzqhZdBLwsrXMA8GTg3rzqYWady3HErPfkeSnnUOAtwE2Srk/zTgbOBs6WtA74HXDs1OZXM7PEccSsx+R5V04Z0DSL35zXfs2se3RzHCmVShSLxceeUl4oFPxwUDP8dGEzs6YrlUoUCgXGxsYYHBykXC4zMjIC4OTEep6HpDcza7JiscjY2BhDQ0PMnz+foaEhxsbGKBaLra6aWcs5MTEza7JKpcLg4ODj5g0ODlKpVFpUI7P24cTEzKzJ+vr6KJfLj5tXLpfp6+trUY3M2ocTEzOzJisUCoyMjDA+Ps7WrVsZHx9nZGSEQqHQ6qqZtZw7v5qZNdlkB9fR0dHH7sopFovu+GqGExMzs5YYHh52ImJWgy/lmJmZWdtwYmJmZmZtw4mJmZmZtQ0nJmZmZtY2nJiYmZlZ23BiYmZmZm3DiYmZmZm1DScmZmZm1jZyS0wkLZU0Lqki6WZJx09ZfpKkkLQorzpYa0hq+GVWi+OIWe/Jc+TXh4ETI+I6SQuAtZIui4gfSFoKvAJYn+P+rUUiouZ8SdMuM5uG44hZj8mtxSQiNkXEdWl6M1ABlqTFnwL+FvD/UmY2LccRs97TlGflSFoBPBe4RtJrgTsj4oZ6TfiSVgGrABYvXszExEQTamp58/do28txxKw3KO+mdUm7A5cDReASYBx4ZUT8StLtwEBE3FuvjIGBgVizZk2u9axne/pA+JLFE/lSTneQtDYiBpq8z46PI2a2Tb04kutdOZLmAxcAX4mIC4H9gWcAN6Rgsh9wnaSn51mPHRURNV8zLTOzHdctccTMZie3SznKmhnGgEpEnAEQETcBe1etczuzONNpphXLlnDHhrtmvf5sW1OWL92X29ffub3VMutJnRpHzGz7NZyYSNoT+OuIKM6w6qHAW4CbJF2f5p0cEd9udJ/NdMeGu3jkwrfOebnzjvrinJdp1qm6PY6Y2fabNjFJt+L9PbAvcBFwPvBhsiBRmqngiCgDdZsTImLF7KtqZp3GccTMGlWvxeQ8ss5mFwBHAFcDNwMHR8TPmlA3M+t8jiNm1pB6nV+fFhGnRsR3IuK9wGLgOAcTM2uA48g0SqUS/f39zJs3j/7+fkqlGRuQzHpC3T4mkp7KtmbUnwFPkbQbQETcl3PdzKwLOI48UalUolAoMDY2xuDgIOVymZGREQCGh4dbXDuz1qqXmOwBrOXx13evS/8G8My8KmVmXcNxpIZiscjY2BhDQ0MADA0NMTY2xujoqBMT63nTJibuUGbWHN08gJ/jSG2VSoXBwcHHzRscHKRSqbSoRmbtY6ZLOU8GjgEOIju7+QFwfkQ81IS6mfWEbn/ooePIE/X19VEulx9rMQEol8v09fW1sFZm7WHazq+Sfp8sgKwke3rnxjR9s6SDmlE5M+tsjiO1FQoFRkZGGB8fZ+vWrYyPjzMyMkKhUGh11cxarl6LyWrgryLisuqZkg4HzgSGam7V4eKUhXDDBfmUa9Z7ejKOzGSyH8no6CiVSoW+vj6KxaL7l5hR5yF+kn4YEQdOs6wSEU1rc2zmw7ck5Tbyazc0y++Ibrk00SzterwaeYhfr8YRM6tvex/it5OknWsUtgs5PmPHzLqK44iZNaReYnIecIGkFZMz0vTXgC/lWy0z6xKOI2bWkHq3C39E0ruBKyQ9Jc3+NfCJiFjdlNqZWUdzHDGzRtVtSo2IM4EzJS1I7zc3pVZm1jUcR8ysEfUu5SBpnqRFEbE5IjZLerKkVZI8CpCZzYrjiJk1ot44Jm8C7gNulHS5pCHgNuBPyAZLMjOry3HEzBpV71LOB4HnRcSPJf0hcBXwpoj4xmwKlrSUrOPb04FHgbMi4jOSPg68Bvgd8BPgrRFx/w58BjNrX44jZtaQepdyfhcRPwaIiOuAn842mCQPAyemcQpeBPx1GgXyMqA/Ig4GbgE+sH1VN7MO4DhiZg2p12Kyt6T3Vb3fvfp9RJxRr+CI2ARsStOb0/XkJRFxadVqVwOvb7zaZtYhHEfMrCH1EpMvAAvqvJ+1NG7Bc4Frpix6G/DVabZZBawCWLx4MRMTE9uz67bSDZ9hR/kYNKYLjpfjiJk1ZNoh6etuJO0WEb+e5bq7A5cDxYi4sGp+ARgAjooZKuEh6btDuw6x3q7a9Xg1MiT9DOV0bRwxs/q2d0h6JC2RNJAeW46kvSX9I3DrLHc8H7gA+MqUYHIs8GrgmJmCibW3ZcuXImlWL2DW6y5bvrTFn8zmiuOIzYVSqUR/fz/z5s2jv7+fUqnU6ipZTqa9lCPpBKAA/BjYWdJngDPIesg/b6aClf1PNAZUqq8jSzoCeD9wWET8Zodqby23Yf1Gvv6TL895uUfv/+Y5L9Oaz3HE5kKpVKJQKDA2Nsbg4CDlcpmRkREAP5G5C9VrMVkFPCciXgz8Kdm14VdFxHtTh7SZHAq8BXiZpOvT60iyR50vAC5L8z6/Yx/BzNqY44jtsGKxyNjYGENDQ8yfP5+hoSHGxsYoFoutrlrb6uQWpnqdX7dExH0AEbFe0i0RcfVsC46IMqAai77dYB2bavnSfZl31BdzKdesB/VkHLG5ValUGBwcfNy8wcFBKhUPHlxLp7cw1UtM9pP02ar3e1e/j4j35Fet1rl9/Z2zXrddOyeatZGejCM2t/r6+iiXywwNDT02r1wu09fX18Jata/qFibgsRam0dHRjk9M/mbK+7V5VsTMupLjiO2wQqHAyMjIE1oAfCmntk5vYZo2MYmIc6dbJqnuU4nNzMBxxObG5Fn+6OgolUqFvr4+isViR5z9t0KntzDVe4hfuWr6S1MWX5tbjcysaziO2FwZHh5m3bp1PPLII6xbt85JSR2TLUzj4+Ns3bqV8fFxRkZGKBQKra7arNQ7Y9mtavqgKctqdUYzM5vKccSsyTq9haleYlKvV6d7fJrZbDiOmLXA8PBwxyQiU9Ubx2RPSf9b0p+l6aPS68+APZpUPzPrbI4jNic6eVwOa0y9FpPLgddWTb+matkVudXIzLqJ44jtsE4fl8MaU++unLl/kp2Z9RTHEZsLnT4uhzWm7kP8zMzMWq3Tx+WwxjgxMTOztjY5Lke1ThqXwxrjxMTMzNpap4/LYY2Zto+JpJfW2zAi3HHNzOpyHNlGanzYFj+LK9Pp43JYYxp5Vg5k4w78L2A/YF4uNTKzbuI4kkyXZPhhoLPTyeNyWGPq3ZVTfVsfkgaBArAJeHfO9TKzLuA4YmaNmrGPiaSXS5oAPgycEREviohvzWK7pZLGJVUk3Szp+DT/aZIuk3Rr+vepO/wpzKyt9VocWbFsCZJm9QJmve6KZUta/MnM8levj8mryM5sfgUUIuJ7DZb9MHBiRFwnaQGwVtJlwHHAf0bE6ZL+Dvg74P3bVXsza2u9Gkfu2HAXj1w490O4zDvqi3Neplm7qdfH5FvARuAXwPundtyKiNfW2qhq+Say5loiYrOkCrAEeB2wMq12LjBBGwUUM5tTjiNm1pB6icnQXO1E0grgucA1wOIUbIiITZL2nmabVcAqgMWLFzMxMTFX1ZlT7VqvbuBj2xXHwHFkjnXDZzCrp17n18trzZe0FHgT2XMvZiRpd+AC4ISIeGC2t8xFxFnAWQADAwOxcuXKWW3XbO1ar27gY9v5x8BxZO51w2cwq2dWA6xJWiTpryRdQdZkuniW280nCyZfiYgL0+y7Je2Tlu8D3NNwrc2s4ziOmNlsTJuYSFog6S8kXQJcCzwLeGZE7B8RJ81UsLJTmjGgEhFnVC36JnBsmj4W+Lftrr2ZtTXHETNrVL0+JveQBZIPAuWICEn/u4GyDwXeAtwk6fo072TgdOBrkkaA9cDRDdfazDqF44iZNaReYnIy2TXg/wOcL+mrjRQcEWVgugvBL2+kLDPrWI4jZtaQaS/lRMSnIuKFwGvJAsNFwL6S3i/pgCbVz8w6mOOImTVqxs6vEXFbRBQj4g+A5wN7AP+Re83MutC++y3LZURQSey737IWf7rpOY6Y2WzVG/n1wIj4YZreOSIeioibyK71zjiUtJk90aY7N3DQhy/Npeyb//6VuZS7I3o1jsQpC+GGC/Ip12wWSqUSxWLxsacxFwqFjnkIYr0+JucDf5imr6qaBvjclPfWo+KUhXDeu/Ip17pBT8YRnfZAbkPSx6lzXqx1mVKpRKFQYGxsjMHBQcrlMiMjIwAdkZzUS0w0zXSt99ajdNoDfP0nX57zco/e/80OwN3BccSsyYrFImNjYwwNZQMvDw0NMTY2xujoaEckJvX6mMQ007Xem5nV4jhi1mSVSoXBwcHHzRscHKRSqbSoRo2p12Kyn6TPkp3VTE6T3vvZ22Y2G44jZk3W19dHuVx+rMUEoFwu09fX18JazV69xORvqqbXTFk29b2ZWS2OI2ZNVigUGBkZeUIfk2Kx2OqqzUq9h/id28yKmFn3cRwxa77JfiSjo6OP3ZVTLBY7on8J1L9d+Jv1NoyI1859ddpTvSeZTrcswpfPzRxHzFpjeHi4YxKRqepdynkxsAEoAdfQwz3onWRMb+my/Th6/zfnUq51BccRM2tIvcTk6cArgGHgz4F/B0oRcXMzKmadYf0dG2a9riQneb3HccTMGlKvj8kjwCXAJZJ2JgssE5L+ISJWN6uCZta5ejWOLF+6L/OO+mIu5Zp1u3otJqRA8iqyYLIC+CxwYf7VMrNu0Ytx5Pb1d856Xbckmj1evc6v5wL9ZA/aOi0i1jWtVmbWFRxHzKxR9UZ+fQtwAHA8cKWkB9Jrs6QHZipY0tmS7pG0rmreIZKulnS9pDWSXrDjH8HM2pjjiJk1ZNrEJCJ2iogF6bWw6rUgImbzhLVzgCOmzPsY2VnTIcCH0nsz61KOI2bWqHotJjskIq4A7ps6G5gMRnsAd+W1fzPrfI4jZr2nbufXHJwAfEfSJ8iSoj+abkVJq4BVAIsXL2ZiYqIZ9bOc+XvMV48c3xPosjjSrvUyawXl2Rtc0grg4ojoT+8/C1weERdIegOwKiIOn6mcgYGBWLPGj9XodL77IDsGB3340lzKvvnvX9mU4ytpbUQM5L6jbftbQRfHEf9dWC+qF0dyu5QzjWPZdpvg1wF3WjOzRjmOmHWxZicmdwGHpemXAbc2ef9m1vkcR3qApIZf1h1y62MiqQSsBBZJ2gicArwd+IykJwFbSNd+zXpFnLIQHn59PoWfMpubXDqL40hvWLZ8KRvWb9zhcqYmJ0uX7dfQYzOsPeSWmETEdI81fF5e+zRrdzrtgXz7mJyaS9Et4zjSGzas38jXf/LlOS83jweMWv6afSnHzMzMbFpOTMzMzKxtODExMzOztuHExMzMzNqGExMzMzNrG05MzMzMrG04MTEzM7O20eyH+JmZ9aR6I5NOt8zP0LFe5MTEzKwJnGRYnrZnSP52/U06MbE55zNDM2tEnLIQzntXPuV2oX33W8amO3d8qP1a8XifJUu5a+P6HS57RzgxsTnnJMPMGqHTHshtSPpue0wDwKY7N+T6aItWc+dXMzMzaxtOTMzMzKxtODExMzOztuHExMzMzNpGbp1fJZ0NvBq4JyL6q+aPAu8GHgb+PSL+Nq86mLWbfZYsza1z2T5LluZSbis5jpj1njzvyjkHOBM4b3KGpCHgdcDBEfGQpL1z3L9Z22nkNjxJvsPJccSs5+SWmETEFZJWTJn9V8DpEfFQWueevPZvZp3PcaQ3LF22H0fv/+ZcyrXO0+xxTA4AXiKpCGwBToqI79daUdIqYBXA4sWLmZiYaFolzdqFf/c1OY50mfPO/dKs1x0aGmJ8fHzW63fjdx6nLISHX59P4acsbPkxU55NxelM5+LJa8OS1gH/BRwPPB/4KvDMmKESAwMDsWbNmtzqadaO2vVSjqS1ETHQxP2twHHEknb9u2gmSbkOsNaM41svjjT7rpyNwIWRuRZ4FFjU5DqYWWdzHDHrYs1OTC4CXgYg6QDgycC9Ta6DmXW2i3AcMetaed4uXAJWAoskbQROAc4Gzk5Nsb8Djp2p+dXMepfjiFnvyfOunOFpFs1912sz60qOI2ZP1O3jIfnpwmZmZh2k28dD8pD0ZmZm1jacmJiZmVnbcGJiZmZmbcN9TMzMzDqcpIaXtWvfEycmZmZmHa5dk4zt4Us5ZmZm1jacmJiZmVnbcGJiZmZmbcOJiZmZmbUNd341M7O20013mVhjnJiYmVnbcZLRu3wpx8zMzNqGExMzMzNrG76UY9ZivpZuZrZNbi0mks6WdI+kdTWWnSQpJC3Ka/9mnSIiGn71CscRs96T56Wcc4Ajps6UtBR4BbA+x32bWXc4B8cRs56SW2ISEVcA99VY9Cngb4HeOe0zs+3iOGLWe5ra+VXSa4E7I+KGZu7XzLqH44hZd2ta51dJTwEKwCtnuf4qYBXA4sWLmZiYyK9yZtYRHEfMul8z78rZH3gGcEO602A/4DpJL4iIn01dOSLOAs4CGBgYiJUrVzaxqmbWphxHzLpc0xKTiLgJ2HvyvaTbgYGIuLdZdTCzzuY4Ytb98rxduARcBTxH0kZJI3nty8y6k+OIWe/JrcUkIoZnWL4ir32bWXdwHDHrPR6S3szMrMuUSiX6+/uZN28e/f39lEqlVldp1jwkvZmZWRcplUoUCgXGxsYYHBykXC4zMpJdBR0ertsI2RbcYmJmZtZFisUiY2NjDA0NMX/+fIaGhhgbG6NYLLa6arPixMTMzKyLVCoVBgcHHzdvcHCQSqXSoho1xomJmZlZF+nr66NcLj9uXrlcpq+vr0U1aowTEzMzsy5SKBQYGRlhfHycrVu3Mj4+zsjICIVCodVVmxV3fjUzM+sikx1cR0dHqVQq9PX1USwWO6LjKzgxMTMz6zrDw8Mdk4hM5Us5ZmZm1jacmJiZmVnbcGJiZmZmbcOJiZmZmbUNJyZmZmbWNpyYmJmZWdtwYmJmZmZtw4mJmZmZtY3cEhNJZ0u6R9K6qnkfl/RDSTdK+oakPfPav5l1PscRs96TZ4vJOcARU+ZdBvRHxMHALcAHcty/mXW+c3AcMespuSUmEXEFcN+UeZdGxMPp7dXAfnnt38w6n+OIWe9p5bNy3gZ8dbqFklYBq9LbByX9qCm1aswi4N5WV6KD+Hg1pl2P1/JWV6CK40jv8fFqTLser2njiCIit71KWgFcHBH9U+YXgAHgqMizAjmTtCYiBlpdj07h49UYH6+M44hV8/FqTCcer6a3mEg6Fng18PJODiZm1jqOI2bdq6mJiaQjgPcDh0XEb5q5bzPrDo4jZt0tz9uFS8BVwHMkbZQ0ApwJLAAuk3S9pM/ntf8mOavVFegwPl6N6fnj5ThiNfh4NabjjleufUzMzMzMGuGRX83MzKxtODExMzOzttGViYmkB2vMe6ekv2hFfbpd9fGWdKSkWyUtk3SqpN9I2nuadUPSJ6venyTp1KZVfApJj6Q+C+skfWuuhjqXdJykM+eirCnlTkj6Uarz9ZJeP9f7SPtZIenP8yi7nTmONJfjyIzl9kwc6crEpJaI+HxEnJdX+cpMezwlzctr3+1C0suB1cAREbE+zb4XOHGaTR4CjpK0qBn1m4XfRsQhabyM+4C/bnWFZuGYVOdDIuL/zWYDSY3ejbcC6LnEpBbHkfw5jrREW8WRnklMUtZ9UpqekPRRSddKukXSS9L8eekBYd9PDwh7R5q/u6T/lHSdpJskvS7NXyGpIumfgeuApVP2ebukD0kqA0dLeqWkq1I5X5e0e1rvSGUPJStL+qyki5t4aOZEOoZfAF4VET+pWnQ28EZJT6ux2cNkPcbf24QqNuoqYAmApBdIulLS/6R/n5PmHyfpQkmXpLO7j01uLOmt6bd1OXBo1fzl6bd0Y/p3WZp/jqT/I2lc0m2SDlP2ALuKpHNmW2lJT5N0USr/akkHp/mnSjpL0qXAeZL2knRB+q1/X9Khab3Dqs6c/kfSAuB04CVpXjt+V03jOJIvxxHHEQAioutewIM15p0KnJSmJ4BPpukjge+m6VXAB9P0zsAa4Blk470sTPMXAT8GRJYBPgq8aJp63A78bdV2VwC7pffvBz4E7AJsAJ6R5pfIRrls+XFs4HhvJTszOLjWMU+f87Sp3w3wILAwHac90rqntvp3A8wDvk52xkaq45PS9OHABWn6OOC2VPddgDvI/lPZB1gP7AU8GfgecGba5lvAsWn6bcBFafoc4F/T7+p1wAPAH5CdPKwFDqlR3wngR8D16fV7ZGeap6TlLwOur/ou1gK7pvfnA4NpehlQqarfoWl69/TbX9lpv8m5/D3U+k1XHX/Hkbk73o4jjiNEREufldNqF6Z/15IFBoBXAgdr2zW2PYBnAxuBf5T0UrIAsgRYnNa5IyKurrOfyed4vAj4feB7kiD7oV0FHAjcFhE/TeuV2PZsj06xFbgSGAGOr7H8s8D1qroOPCkiHpB0HvAe4Le51nJmu0q6nuz3sJbsKbaQ/Q7OlfRsIID5Vdv8Z0T8CkDSD8ie/7AImIiIn6f5XwUOSOu/GDgqTX8J+FhVWd+KiJB0E3B3RNyUtr851en6GnU+JiLWTL6RNAj8GUBE/Jek35O0R1r8zYiYPMaHA7+ffosAC9NZzfeAMyR9BbgwIjZWrWNP5DgydxxHcByBHrqUU8ND6d9H2DYCroDR2Hat7RkRcSlwDFnW+ryIOAS4myyzBfj1DPuZXC7gsqqyfz8iRtL8Tvco8Abg+ZJOnrowIu4ny6zfNc32nyYLRrvlVL/Z+m36fpeTBfzJa8MfBsYju2b8GrZ997DtdwSP/y3NdoCg6vUmy3p0SrmPMvtRmmv9nib3Uf1b3Ql4cdXvcUlEbI6I04G/BHYFrpZ04Cz326scR+aO40im5+NILycmtXwH+CtJ8wEkHSBpN7JM956I2CppiO17uurVwKGSnpXKfoqkA4AfAs9U9qAygDfu6IdohciGBn81cIyy0TmnOgN4BzX+MCLiPuBrZEGl5dKZy3uAk9JvYQ/gzrT4uFkUcQ2wMp1lzAeOrlp2JfCmNH0MUJ6TSm9zRSoXSSuBeyPigRrrXQq8e/KNpEPSv/tHxE0R8VGySxAHApvJRlq12XEc2U6OI4/Ts3GkWxOTpygbvnry9b5Zbvd/gR8A10laB/wL2R/AV4ABSWvIvqwfNlqh1Bx3HFCSdCNZgDkwNYm9C7hEWee2u4HJJr0BSf+30X21SgoMRwAfVOrYV7XsXuAbZNfca/kkWdNlW4iI/wFuIPvj/xjwT5K+R3bdeKZtN5Fdi70K+C5Zh8ZJ7wHemn4Db6F2k/WOOJXst3ojWWezY6dZ7z2T66Wm43em+Scou83xBrIm8f8AbgQelnTDjJ3WuovjSAs4jjy2bc/GEQ9J3wYk7R4RDyq7CPc54NaI+FSr62VmncNxxLpFt7aYdJq3p85SN5M19/1La6tjZh3IccS6gltMzMzMrG24xcTMzMzahhMTMzMzaxtOTMzMzKxtODExMzOztuHExMzMzNrG/wfJHkHu30Ge8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 648x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOTS:\n",
    "    \n",
    "# https://matplotlib.org/stable/gallery/statistics/boxplot_color.html\n",
    "\n",
    "\n",
    "labels = ['Linear reg.', 'KNN', 'Random Forest']\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))\n",
    "\n",
    "# Spike counts:\n",
    "bplot1 = ax1.boxplot(df_rmse_te[['linreg_spikes', 'knn_spikes', 'rf_spikes']],\n",
    "                     vert=True,  # vertical box alignment\n",
    "                     patch_artist=True,  # fill with color\n",
    "                     labels=labels)  # will be used to label x-ticks\n",
    "ax1.set_title('Models with spike counts')\n",
    "ax1.set_ylabel('MEAN ERROR')\n",
    "\n",
    "# Sentences:\n",
    "bplot2 = ax2.boxplot(df_rmse_te[['linreg_sents', 'knn_sents', 'rf_sents']],\n",
    "                     #notch=True,  # notch shape\n",
    "                     vert=True,  # vertical box alignment\n",
    "                     patch_artist=True,  # fill with color\n",
    "                     labels=labels)  # will be used to label x-ticks\n",
    "ax2.set_title('Models with w2v sentences')\n",
    "\n",
    "colors = ['#fdae61', '#abdda4', '#2b83ba']\n",
    "for bplot in (bplot1, bplot2):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "# Horizontal gridlines\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.yaxis.grid(True)\n",
    "    # ax.set_xlabel('Models with spike counts')\n",
    "    # ax.set_ylabel('Models with w2v sentences')\n",
    "    ax.set_ylim(12, 30)\n",
    "\n",
    "#plt.title('Mean errors over 10 folds')\n",
    "plt.ylabel('MEAN ERROR')  \n",
    "# plt.savefig(\"simple_models_rsmes.svg\")\n",
    "# plt.savefig(\"simple_models_rsmes.png\")\n",
    "# plt.savefig(\"simple_models_rsmes.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d643c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d25673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188529fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f339fa1",
   "metadata": {},
   "source": [
    "Combine all predictions of all models of each fold to dataframe (to make comparing them easier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58dc3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_preds_df(cols, is_spikes, dfs_orig):\n",
    "    dfs_list = []\n",
    "    for i, models_preds in enumerate(dfs_orig): # model\n",
    "        for k in range(10): # fold\n",
    "            fold_preds = models_preds[k]\n",
    "            if i==0:\n",
    "                if is_spikes==True:\n",
    "                    df_preds_fold = fold_preds[['x', 'y']]\n",
    "                else:\n",
    "                    df_preds_fold = fold_preds[['sent', 'length', 'x', 'y']]\n",
    "            else:\n",
    "                df_preds_fold = dfs_list[k]\n",
    "            model_name = cols[i].split('_')[0]\n",
    "            df_preds_fold[model_name + \"_x\"] = fold_preds['pred_x']\n",
    "            df_preds_fold[model_name + \"_y\"] = fold_preds['pred_y']\n",
    "            df_preds_fold[model_name + \"_dist\"] = fold_preds['dist']  \n",
    "            if i==0:\n",
    "                dfs_list.append(df_preds_fold )\n",
    "            else:\n",
    "                dfs_list[k] = df_preds_fold      \n",
    "    return dfs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d50de93",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renat\\AppData\\Local\\Temp/ipykernel_17556/2269939825.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_preds_fold[model_name + \"_x\"] = fold_preds['pred_x']\n",
      "C:\\Users\\renat\\AppData\\Local\\Temp/ipykernel_17556/2269939825.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_preds_fold[model_name + \"_y\"] = fold_preds['pred_y']\n"
     ]
    }
   ],
   "source": [
    "pred_dfs_spikes = [pred_linreg_spikes, pred_knn_spikes, pred_rf_spikes]\n",
    "pred_dfs_sents =  [pred_linreg_sents, pred_knn_sents, pred_rf_sents]\n",
    "cols_spikes = ['linreg_spikes', 'knn_spikes', 'rf_spikes']\n",
    "cols_sents = ['linreg_sents', 'knn_sents', 'rf_sents']\n",
    "\n",
    "dfs_pred_spikes = make_single_preds_df(cols_spikes, True, pred_dfs_spikes)\n",
    "dfs_pred_sents = make_single_preds_df(cols_sents, False, pred_dfs_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a76550a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>length</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>linreg_x</th>\n",
       "      <th>linreg_y</th>\n",
       "      <th>linreg_dist</th>\n",
       "      <th>knn_x</th>\n",
       "      <th>knn_y</th>\n",
       "      <th>knn_dist</th>\n",
       "      <th>rf_x</th>\n",
       "      <th>rf_y</th>\n",
       "      <th>rf_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[35, 49, 4, 4, 4, 6, 6, 5, 20, 4, 56, 6, 55, 3...</td>\n",
       "      <td>86</td>\n",
       "      <td>52.171690</td>\n",
       "      <td>45.150960</td>\n",
       "      <td>44.240630</td>\n",
       "      <td>48.140888</td>\n",
       "      <td>8.475930</td>\n",
       "      <td>53.278613</td>\n",
       "      <td>52.124170</td>\n",
       "      <td>7.060520</td>\n",
       "      <td>49.382577</td>\n",
       "      <td>57.502042</td>\n",
       "      <td>12.662084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4, 35, 5, 55, 1, 49, 35, 35, 6, 6, 35, 35, 19...</td>\n",
       "      <td>88</td>\n",
       "      <td>46.543049</td>\n",
       "      <td>40.373398</td>\n",
       "      <td>44.501304</td>\n",
       "      <td>48.616396</td>\n",
       "      <td>8.492098</td>\n",
       "      <td>52.443910</td>\n",
       "      <td>52.267888</td>\n",
       "      <td>13.277765</td>\n",
       "      <td>49.421864</td>\n",
       "      <td>56.665527</td>\n",
       "      <td>16.544517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[35, 4, 49, 4, 35, 4, 35, 35, 55, 4, 55, 1, 4,...</td>\n",
       "      <td>105</td>\n",
       "      <td>40.090321</td>\n",
       "      <td>35.661596</td>\n",
       "      <td>42.838413</td>\n",
       "      <td>48.583924</td>\n",
       "      <td>13.211304</td>\n",
       "      <td>52.903323</td>\n",
       "      <td>55.180361</td>\n",
       "      <td>23.348559</td>\n",
       "      <td>49.117442</td>\n",
       "      <td>57.061790</td>\n",
       "      <td>23.226219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[20, 35, 4, 4, 15, 4, 4, 49, 56, 4, 5, 35, 52,...</td>\n",
       "      <td>103</td>\n",
       "      <td>36.007021</td>\n",
       "      <td>32.948307</td>\n",
       "      <td>44.084908</td>\n",
       "      <td>47.774432</td>\n",
       "      <td>16.883905</td>\n",
       "      <td>52.058908</td>\n",
       "      <td>56.549919</td>\n",
       "      <td>28.542935</td>\n",
       "      <td>49.323082</td>\n",
       "      <td>57.459932</td>\n",
       "      <td>27.895112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[19, 4, 56, 6, 19, 5, 19, 49, 49, 35, 35, 55, ...</td>\n",
       "      <td>99</td>\n",
       "      <td>35.142944</td>\n",
       "      <td>32.040542</td>\n",
       "      <td>44.295923</td>\n",
       "      <td>46.493481</td>\n",
       "      <td>17.107440</td>\n",
       "      <td>51.326846</td>\n",
       "      <td>52.418515</td>\n",
       "      <td>26.022692</td>\n",
       "      <td>49.802494</td>\n",
       "      <td>57.792190</td>\n",
       "      <td>29.631905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent  length          x  \\\n",
       "0  [35, 49, 4, 4, 4, 6, 6, 5, 20, 4, 56, 6, 55, 3...      86  52.171690   \n",
       "1  [4, 35, 5, 55, 1, 49, 35, 35, 6, 6, 35, 35, 19...      88  46.543049   \n",
       "2  [35, 4, 49, 4, 35, 4, 35, 35, 55, 4, 55, 1, 4,...     105  40.090321   \n",
       "3  [20, 35, 4, 4, 15, 4, 4, 49, 56, 4, 5, 35, 52,...     103  36.007021   \n",
       "4  [19, 4, 56, 6, 19, 5, 19, 49, 49, 35, 35, 55, ...      99  35.142944   \n",
       "\n",
       "           y   linreg_x   linreg_y  linreg_dist      knn_x      knn_y  \\\n",
       "0  45.150960  44.240630  48.140888     8.475930  53.278613  52.124170   \n",
       "1  40.373398  44.501304  48.616396     8.492098  52.443910  52.267888   \n",
       "2  35.661596  42.838413  48.583924    13.211304  52.903323  55.180361   \n",
       "3  32.948307  44.084908  47.774432    16.883905  52.058908  56.549919   \n",
       "4  32.040542  44.295923  46.493481    17.107440  51.326846  52.418515   \n",
       "\n",
       "    knn_dist       rf_x       rf_y    rf_dist  \n",
       "0   7.060520  49.382577  57.502042  12.662084  \n",
       "1  13.277765  49.421864  56.665527  16.544517  \n",
       "2  23.348559  49.117442  57.061790  23.226219  \n",
       "3  28.542935  49.323082  57.459932  27.895112  \n",
       "4  26.022692  49.802494  57.792190  29.631905  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: df of sentences models of fold 0:\n",
    "dfs_pred_sents[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f873a4c1",
   "metadata": {},
   "source": [
    "**Save results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea57fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_results + \"dfs_preds_folds.plk\", \"wb\") as fout:\n",
    "    plk.dump([dfs_pred_spikes, dfs_pred_sents], fout) \n",
    "\n",
    "df_rmse_te.to_csv(path_to_results + \"rmses_test.csv\", sep=\",\", index=False, encoding=\"utf-8\") \n",
    "df_rmse_tr.to_csv(path_to_results + \"rmses_train.csv\", sep=\",\", index=False, encoding=\"utf-8\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72ef79",
   "metadata": {},
   "source": [
    "**Load results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c9d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rmse_te = pd.read_csv(path_to_results + \"rmses_test.csv\", sep=\",\", encoding = \"UTF-8\") \n",
    "# df_rmse_tr = pd.read_csv(path_to_results + \"rmses_train.csv\", sep=\",\", encoding = \"UTF-8\") \n",
    "\n",
    "# with open(path_to_results + \"dfs_preds_folds.plk\", \"rb\") as fin:\n",
    "#     loaded = plk.load(fin)\n",
    "#     dfs_pred_spikes =loaded[0] \n",
    "#     dfs_pred_sents = loaded[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388f51b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
